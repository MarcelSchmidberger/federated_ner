{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.style.use('seaborn-pastel')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for the Model\n",
    "parameters = OrderedDict()\n",
    "parameters['train'] = \"../data/eng.train\" #Path to train file\n",
    "parameters['dev'] = \"../data/eng.testa\" #Path to test file\n",
    "parameters['test'] = \"../data/eng.testb\" #Path to dev file\n",
    "parameters['tag_scheme'] = \"BIOES\" #BIO or BIOES\n",
    "parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
    "parameters['zeros'] =  True # Boolean variable to control replacement of  all digits by 0 \n",
    "parameters['char_dim'] = 30 #Char embedding dimension\n",
    "parameters['word_dim'] = 100 #Token embedding dimension\n",
    "parameters['word_lstm_dim'] = 200 #Token LSTM hidden layer size\n",
    "parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n",
    "parameters['embedding_path'] = \"../data/glove.6B.100d.txt\" #Location of pretrained embeddings\n",
    "parameters['all_emb'] = 1 #Load all embeddings\n",
    "parameters['crf'] =1 #Use CRF (0 to disable)\n",
    "parameters['dropout'] = 0.5 #Droupout on the input (0 = no dropout)\n",
    "parameters['epoch'] =  1 #Number of epochs to run\"\n",
    "parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
    "parameters['name'] = \"self-trained-model\" # Model name\n",
    "parameters['gradient_clip']=5.0\n",
    "parameters['char_mode']=\"CNN\"\n",
    "models_path = \"../models/\" #path to saved models\n",
    "\n",
    "#GPU\n",
    "parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
    "use_gpu = parameters['use_gpu']\n",
    "\n",
    "parameters['reload'] = False \n",
    "\n",
    "#Constants\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to files \n",
    "#To stored mapping file\n",
    "mapping_file = '../data/mapping.pkl'\n",
    "\n",
    "#To stored model\n",
    "name = parameters['name']\n",
    "model_name = models_path + name #get_name(parameters)\n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
    "test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
    "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word, Character and Tag Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and mapping of characters, sorted by frequency.\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    print(\"Found %i unique characters\" % len(dico))\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17493 unique words (203621 in total)\n",
      "Found 75 unique characters\n",
      "Found 10 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
    "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(x,lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041 / 3250 / 3453 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        # Skip characters that are not in the training set\n",
    "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "                 for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "print(\"{} / {} / {} sentences in train / dev / test.\".format(len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'str_words': ['EU',\n",
       "   'rejects',\n",
       "   'German',\n",
       "   'call',\n",
       "   'to',\n",
       "   'boycott',\n",
       "   'British',\n",
       "   'lamb',\n",
       "   '.'],\n",
       "  'words': [944, 15473, 198, 590, 8, 3848, 207, 6233, 2],\n",
       "  'chars': [[31, 48],\n",
       "   [6, 0, 52, 0, 12, 2, 7],\n",
       "   [42, 0, 6, 14, 1, 3],\n",
       "   [12, 1, 9, 9],\n",
       "   [2, 5],\n",
       "   [21, 5, 19, 12, 5, 2, 2],\n",
       "   [36, 6, 4, 2, 4, 7, 11],\n",
       "   [9, 1, 14, 21],\n",
       "   [18]],\n",
       "  'tags': [2, 0, 4, 0, 0, 0, 4, 0, 0]},\n",
       " {'str_words': ['Peter', 'Blackburn'],\n",
       "  'words': [741, 1797],\n",
       "  'chars': [[40, 0, 2, 0, 6], [36, 9, 1, 12, 27, 21, 13, 6, 3]],\n",
       "  'tags': [1, 1]},\n",
       " {'str_words': ['BRUSSELS', '0000-00-00'],\n",
       "  'words': [678, 23],\n",
       "  'chars': [[36, 33, 48, 25, 25, 31, 37, 25],\n",
       "   [8, 8, 8, 8, 22, 8, 8, 22, 8, 8]],\n",
       "  'tags': [3, 0]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 pretrained embeddings.\n"
     ]
    }
   ],
   "source": [
    "all_word_embeds = {}\n",
    "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == parameters['word_dim'] + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "#Intializing Word Embedding Matrix\n",
    "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n",
    "\n",
    "for w in word_to_id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_id:  17493\n"
     ]
    }
   ],
   "source": [
    "with open(mapping_file, 'wb') as f:\n",
    "    mappings = {\n",
    "        'word_to_id': word_to_id,\n",
    "        'tag_to_id': tag_to_id,\n",
    "        'char_to_id': char_to_id,\n",
    "        'parameters': parameters,\n",
    "        'word_embeds': word_embeds\n",
    "    }\n",
    "    cPickle.dump(mappings, f)\n",
    "\n",
    "print('word_to_id: ', len(word_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding(input_embedding):\n",
    "    \"\"\"\n",
    "    Initialize embedding\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform(input_embedding, -bias, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear(input_linear):\n",
    "    \"\"\"\n",
    "    Initialize linear transformation\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
    "    nn.init.uniform(input_linear.weight, -bias, bias)\n",
    "    if input_linear.bias is not None:\n",
    "        input_linear.bias.data.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm(input_lstm):\n",
    "    \"\"\"\n",
    "    Initialize lstm\n",
    "    \n",
    "    PyTorch weights parameters:\n",
    "    \n",
    "        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
    "            of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is\n",
    "            `(hidden_size * hidden_size)`\n",
    "            \n",
    "        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
    "            of shape `(hidden_size * hidden_size)`            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Weights init for forward layer\n",
    "    for ind in range(0, input_lstm.num_layers):\n",
    "        \n",
    "        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n",
    "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "        \n",
    "        # Initialize the sampling range\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        \n",
    "        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n",
    "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        # Similar to above but for the hidden-hidden weights of the current layer\n",
    "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        \n",
    "    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n",
    "    if input_lstm.bidirectional:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "\n",
    "    # Bias initialization steps\n",
    "    \n",
    "    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n",
    "    if input_lstm.bias:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "            \n",
    "            # Initializing to zero\n",
    "            bias.data.zero_()\n",
    "            \n",
    "            # This is the range of indices for our forget gates for each LSTM cell\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "            #Similar for the hidden-hidden layer\n",
    "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n",
    "        if input_lstm.bidirectional:\n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fancy math for CRF as they are better than softmax for considering tags from neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    '''\n",
    "    This function calculates the score explained above for the forward algorithm\n",
    "    vec 2D: 1 * tagset_size\n",
    "    '''\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "def argmax(vec):\n",
    "    '''\n",
    "    This function returns the max index in a vector\n",
    "    '''\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "def to_scalar(var):\n",
    "    '''\n",
    "    Function to convert pytorch tensor to a scalar\n",
    "    '''\n",
    "    return var.view(-1).data.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(self, feats, tags):\n",
    "    # tags is ground_truth, a list of ints, length is len(sentence)\n",
    "    # feats is a 2D tensor, len(sentence) * tagset_size\n",
    "    r = torch.LongTensor(range(feats.size()[0]))\n",
    "    if self.use_gpu:\n",
    "        r = r.cuda()\n",
    "        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "    else:\n",
    "        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "\n",
    "    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_alg(self, feats):\n",
    "    '''\n",
    "    This function performs the forward algorithm explained above\n",
    "    '''\n",
    "    # calculate in log domain\n",
    "    # feats is len(sentence) * tagset_size\n",
    "    # initialize alpha with a Tensor with values all equal to -10000.\n",
    "    \n",
    "    # Do the forward algorithm to compute the partition function\n",
    "    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    \n",
    "    # START_TAG has all of the score.\n",
    "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "    \n",
    "    # Wrap in a variable so that we will get automatic backprop\n",
    "    forward_var = autograd.Variable(init_alphas)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "        \n",
    "    # Iterate through the sentence\n",
    "    for feat in feats:\n",
    "        # broadcast the emission score: it is the same regardless of\n",
    "        # the previous tag\n",
    "        emit_score = feat.view(-1, 1)\n",
    "        \n",
    "        # the ith entry of trans_score is the score of transitioning to\n",
    "        # next_tag from i\n",
    "        tag_var = forward_var + self.transitions + emit_score\n",
    "        \n",
    "        # The ith entry of next_tag_var is the value for the\n",
    "        # edge (i -> next_tag) before we do log-sum-exp\n",
    "        max_tag_var, _ = torch.max(tag_var, dim=1)\n",
    "        \n",
    "        # The forward variable for this tag is log-sum-exp of all the\n",
    "        # scores.\n",
    "        tag_var = tag_var - max_tag_var.view(-1, 1)\n",
    "        \n",
    "        # Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
    "    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
    "    alpha = log_sum_exp(terminal_var)\n",
    "    # Z(x)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo(self, feats):\n",
    "    '''\n",
    "    In this function, we implement the viterbi algorithm explained above.\n",
    "    A Dynamic programming based approach to find the best tag sequence\n",
    "    '''\n",
    "    backpointers = []\n",
    "    # analogous to forward\n",
    "    \n",
    "    # Initialize the viterbi variables in log space\n",
    "    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "    \n",
    "    # forward_var at step i holds the viterbi variables for step i-1\n",
    "    forward_var = Variable(init_vvars)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "    for feat in feats:\n",
    "        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
    "        _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n",
    "        next_tag_var = next_tag_var.data.cpu().numpy() \n",
    "        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n",
    "        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
    "        if self.use_gpu:\n",
    "            viterbivars_t = viterbivars_t.cuda()\n",
    "            \n",
    "        # Now add in the emission scores, and assign forward_var to the set\n",
    "        # of viterbi variables we just computed\n",
    "        forward_var = viterbivars_t + feat\n",
    "        backpointers.append(bptrs_t)\n",
    "\n",
    "    # Transition to STOP_TAG\n",
    "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
    "    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
    "    best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
    "    path_score = terminal_var[best_tag_id]\n",
    "    \n",
    "    # Follow the back pointers to decode the best path.\n",
    "    best_path = [best_tag_id]\n",
    "    for bptrs_t in reversed(backpointers):\n",
    "        best_tag_id = bptrs_t[best_tag_id]\n",
    "        best_path.append(best_tag_id)\n",
    "        \n",
    "    # Pop off the start tag (we dont want to return that to the caller)\n",
    "    start = best_path.pop()\n",
    "    assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
    "    best_path.reverse()\n",
    "    return path_score, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_calc(self, sentence, chars, chars2_length, d):\n",
    "    \n",
    "    '''\n",
    "    The function calls viterbi decode and generates the \n",
    "    most probable sequence of tags for the sentence\n",
    "    '''\n",
    "    \n",
    "    # Get the emission scores from the BiLSTM\n",
    "    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
    "    # viterbi to get tag_seq\n",
    "    \n",
    "    # Find the best path, given the features.\n",
    "    if self.use_crf:\n",
    "        score, tag_seq = self.viterbi_decode(feats)\n",
    "    else:\n",
    "        score, tag_seq = torch.max(feats, 1)\n",
    "        tag_seq = list(tag_seq.cpu().data)\n",
    "\n",
    "    return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_features(self, sentence, chars2, chars2_length, d):\n",
    "    \n",
    "    if self.char_mode == 'LSTM':\n",
    "        \n",
    "            chars_embeds = self.char_embeds(chars2).transpose(0, 1)\n",
    "            \n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(chars_embeds, chars2_length)\n",
    "            \n",
    "            lstm_out, _ = self.char_lstm(packed)\n",
    "            \n",
    "            outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
    "            \n",
    "            outputs = outputs.transpose(0, 1)\n",
    "            \n",
    "            chars_embeds_temp = Variable(torch.FloatTensor(torch.zeros((outputs.size(0), outputs.size(2)))))\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                chars_embeds_temp = chars_embeds_temp.cuda()\n",
    "            \n",
    "            for i, index in enumerate(output_lengths):\n",
    "                chars_embeds_temp[i] = torch.cat((outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))\n",
    "            \n",
    "            chars_embeds = chars_embeds_temp.clone()\n",
    "            \n",
    "            for i in range(chars_embeds.size(0)):\n",
    "                chars_embeds[d[i]] = chars_embeds_temp[i]\n",
    "    \n",
    "    \n",
    "    if self.char_mode == 'CNN':\n",
    "        chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
    "\n",
    "        ## Creating Character level representation using Convolutional Neural Netowrk\n",
    "        ## followed by a Maxpooling Layer\n",
    "        chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
    "        chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
    "                                             kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
    "\n",
    "        ## Loading word embeddings\n",
    "    embeds = self.word_embeds(sentence)\n",
    "\n",
    "    ## We concatenate the word embeddings and the character level representation\n",
    "    ## to create unified representation for each word\n",
    "    embeds = torch.cat((embeds, chars_embeds), 1)\n",
    "\n",
    "    embeds = embeds.unsqueeze(1)\n",
    "\n",
    "    ## Dropout on the unified embeddings\n",
    "    embeds = self.dropout(embeds)\n",
    "\n",
    "    ## Word lstm\n",
    "    ## Takes words as input and generates a output at each step\n",
    "    lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "    ## Reshaping the outputs from the lstm layer\n",
    "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
    "\n",
    "    ## Dropout on the lstm output\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "    ## Linear layer converts the ouput vectors to tag space\n",
    "    lstm_feats = self.hidden2tag(lstm_out)\n",
    "    \n",
    "    return lstm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n",
    "    # sentence, tags is a list of ints\n",
    "    # features is a 2D tensor, len(sentence) * self.tagset_size\n",
    "    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n",
    "\n",
    "    if self.use_crf:\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    else:\n",
    "        tags = Variable(tags)\n",
    "        scores = nn.functional.cross_entropy(feats, tags)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
    "                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=25,char_embedding_dim=25, use_gpu=False\n",
    "                 , use_crf=True, char_mode='CNN'):\n",
    "        '''\n",
    "        Input parameters:\n",
    "                \n",
    "                vocab_size= Size of vocabulary (int)\n",
    "                tag_to_ix = Dictionary that maps NER tags to indices\n",
    "                embedding_dim = Dimension of word embeddings (int)\n",
    "                hidden_dim = The hidden dimension of the LSTM layer (int)\n",
    "                char_to_ix = Dictionary that maps characters to indices\n",
    "                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n",
    "                char_out_dimension = Output dimension from the CNN encoder for character\n",
    "                char_embedding_dim = Dimension of the character embeddings\n",
    "                use_gpu = defines availability of GPU, \n",
    "                    when True: CUDA function calls are made\n",
    "                    else: Normal CPU function calls are made\n",
    "                use_crf = parameter which decides if you want to use the CRF layer for output decoding\n",
    "        '''\n",
    "        \n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        #parameter initialization for the model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.use_crf = use_crf\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.out_channels = char_out_dimension\n",
    "        self.char_mode = char_mode\n",
    "\n",
    "        if char_embedding_dim is not None:\n",
    "            self.char_embedding_dim = char_embedding_dim\n",
    "            \n",
    "            #Initializing the character embedding layer\n",
    "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
    "            init_embedding(self.char_embeds.weight)\n",
    "            \n",
    "            #Performing LSTM encoding on the character embeddings\n",
    "            if self.char_mode == 'LSTM':\n",
    "                self.char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)\n",
    "                init_lstm(self.char_lstm)\n",
    "                \n",
    "            #Performing CNN encoding on the character embeddings\n",
    "            if self.char_mode == 'CNN':\n",
    "                self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
    "\n",
    "        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pre_word_embeds is not None:\n",
    "            #Initializes the word embeddings with pretrained word embeddings\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "    \n",
    "        #Initializing the dropout layer, with dropout specificed in parameters\n",
    "        self.dropout = nn.Dropout(parameters['dropout'])\n",
    "        \n",
    "        #Lstm Layer:\n",
    "        #input dimension: word embedding dimension + character level representation\n",
    "        #bidirectional=True, specifies that we are using the bidirectional LSTM\n",
    "        if self.char_mode == 'LSTM':\n",
    "            self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2, hidden_dim, bidirectional=True)\n",
    "        if self.char_mode == 'CNN':\n",
    "            self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
    "        \n",
    "        #Initializing the lstm layer using predefined function for initialization\n",
    "        init_lstm(self.lstm)\n",
    "        \n",
    "        # Linear layer which maps the output of the bidirectional LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
    "        \n",
    "        #Initializing the linear layer using predefined function for initialization\n",
    "        init_linear(self.hidden2tag) \n",
    "\n",
    "        if self.use_crf:\n",
    "            # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "            # Matrix has a dimension of (total number of tags * total number of tags)\n",
    "            self.transitions = nn.Parameter(\n",
    "                torch.zeros(self.tagset_size, self.tagset_size))\n",
    "            \n",
    "            # These two statements enforce the constraint that we never transfer\n",
    "            # to the start tag and we never transfer from the stop tag\n",
    "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    #assigning the functions, which we have defined earlier\n",
    "    _score_sentence = score_sentences\n",
    "    _get_lstm_features = get_lstm_features\n",
    "    _forward_alg = forward_alg\n",
    "    viterbi_decode = viterbi_algo\n",
    "    neg_log_likelihood = get_neg_log_likelihood\n",
    "    forward = forward_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcel/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  \n",
      "/home/marcel/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/marcel/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/ipykernel_launcher.py:30: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/marcel/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/home/marcel/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
     ]
    }
   ],
   "source": [
    "#creating the model using the Class defined above\n",
    "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds,\n",
    "                   use_crf=parameters['crf'],\n",
    "                   char_mode=parameters['char_mode'])\n",
    "print(\"Model Initialized!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7327de1abc59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Reload a saved model, if parameter[\"reload\"] is set to a path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reload'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reload'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloading pre-trained model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/trained-model-cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reload'"
     ]
    }
   ],
   "source": [
    "#Reload a saved model, if parameter[\"reload\"] is set to a path\n",
    "if parameters['reload']:\n",
    "    if not os.path.exists(parameters['reload']):\n",
    "        print(\"downloading pre-trained model\")\n",
    "        model_url=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/trained-model-cpu\"\n",
    "        urllib.request.urlretrieve(model_url, parameters['reload'])\n",
    "    model.load_state_dict(torch.load(parameters['reload']))\n",
    "    print(\"model reloaded :\", parameters['reload'])\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the optimizer\n",
    "#The best results in the paper where achived using stochastic gradient descent (SGD) \n",
    "#learning rate=0.015 and momentum=0.9 \n",
    "#decay_rate=0.05 \n",
    "\n",
    "learning_rate = 0.015\n",
    "momentum = 0.9\n",
    "number_of_epochs = parameters['epoch'] \n",
    "decay_rate = 0.05\n",
    "gradient_clip = parameters['gradient_clip']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "#variables which will used in training process\n",
    "losses = [] #list to store all losses\n",
    "loss = 0.0 #Loss Initializatoin\n",
    "best_dev_F = -1.0 # Current best F-1 Score on Dev Set\n",
    "best_test_F = -1.0 # Current best F-1 Score on Test Set\n",
    "best_train_F = -1.0 # Current best F-1 Score on Train Set\n",
    "all_F = [[0, 0, 0]] # List storing all the F-1 Scores\n",
    "eval_every = len(train_data) # Calculate F-1 Score after this many iterations\n",
    "plot_every = 2000 # Store loss after this many iterations\n",
    "count = 0 #Counts the number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag):\n",
    "    \"\"\"\n",
    "    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n",
    "    as defined in BIOES\n",
    "    \n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # We assume by default the tags lie outside a named entity\n",
    "    default = tags[\"O\"]\n",
    "    \n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                # Initialize chunk for each entity\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                # If chunk class is B, i.e., its a beginning of a new named entity\n",
    "                # or, if the chunk type is different from the previous one, then we\n",
    "                # start labelling it as a new entity\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(model, datas, best_F,dataset=\"Train\"):\n",
    "    '''\n",
    "    The function takes as input the model, data and calcuates F-1 Score\n",
    "    It performs conditional updates \n",
    "     1) Flag to save the model \n",
    "     2) Best F-1 score\n",
    "    ,if the F-1 score calculated improves on the previous F-1 score\n",
    "    '''\n",
    "    # Initializations\n",
    "    prediction = [] # A list that stores predicted tags\n",
    "    save = False # Flag that tells us if the model needs to be saved\n",
    "    new_F = 0.0 # Variable to store the current F1-Score (may not be the best)\n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0. # Count variables\n",
    "    \n",
    "    for data in datas:\n",
    "        ground_truth_id = data['tags']\n",
    "        words = data['str_words']\n",
    "        chars2 = data['chars']\n",
    "        \n",
    "        if parameters['char_mode'] == 'LSTM':\n",
    "            chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "            d = {}\n",
    "            for i, ci in enumerate(chars2):\n",
    "                for j, cj in enumerate(chars2_sorted):\n",
    "                    if ci == cj and not j in d and not i in d.values():\n",
    "                        d[j] = i\n",
    "                        continue\n",
    "            chars2_length = [len(c) for c in chars2_sorted]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2_sorted):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "        \n",
    "        \n",
    "        if parameters['char_mode'] == 'CNN':\n",
    "            d = {} \n",
    "\n",
    "            # Padding the each word to max word size of that sentence\n",
    "            chars2_length = [len(c) for c in chars2]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        \n",
    "        # We are getting the predicted output from our model\n",
    "        if use_gpu:\n",
    "            val,out = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "        else:\n",
    "            val,out = model(dwords, chars2_mask, chars2_length, d)\n",
    "        predicted_id = out\n",
    "    \n",
    "        \n",
    "        # We use the get chunks function defined above to get the true chunks\n",
    "        # and the predicted chunks from true labels and predicted labels respectively\n",
    "        lab_chunks      = set(get_chunks(ground_truth_id,tag_to_id))\n",
    "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
    "                                         tag_to_id))\n",
    "\n",
    "        # Updating the count variables\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds   += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "    \n",
    "    # Calculating the F1-Score\n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "\n",
    "    print(\"{}: new_F: {} best_F: {} \".format(dataset,new_F,best_F))\n",
    "    \n",
    "    # If our current F1-Score is better than the previous best, we update the best\n",
    "    # to current F1 and we set the flag to indicate that we need to checkpoint this model\n",
    "    \n",
    "    if new_F>best_F:\n",
    "        best_F=new_F\n",
    "        save=True\n",
    "\n",
    "    return best_F, new_F, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr):\n",
    "    \"\"\"\n",
    "    shrink learning rate\n",
    "    \"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcel/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 :  0.04514785297465583\n",
      "4000 :  0.02724407303073179\n",
      "6000 :  0.02709344421929273\n",
      "8000 :  0.03952212678907238\n",
      "10000 :  0.04088913593185796\n",
      "12000 :  0.03643619544373375\n",
      "14000 :  0.03413106731536531\n",
      "16000 :  0.030181397807180172\n",
      "18000 :  0.026364404246824312\n",
      "20000 :  0.028853136830869625\n",
      "22000 :  0.027490181042492085\n",
      "24000 :  0.027426365737040822\n",
      "26000 :  0.023112232662762817\n",
      "28000 :  0.020448413371245366\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a1a541e101ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mneg_log_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars2_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars2_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m#we use gradient clipping to avoid exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters['reload']=False\n",
    "print(parameters['reload'])\n",
    "if not parameters['reload']:\n",
    "    tr = time.time()\n",
    "    model.train(True)\n",
    "    for epoch in range(1,number_of_epochs):\n",
    "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "            count += 1\n",
    "            data = train_data[index]\n",
    "\n",
    "            ##gradient updates for each data entry\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = data['words']\n",
    "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "            tags = data['tags']\n",
    "            chars2 = data['chars']\n",
    "            \n",
    "            if parameters['char_mode'] == 'LSTM':\n",
    "                chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "                d = {}\n",
    "                for i, ci in enumerate(chars2):\n",
    "                    for j, cj in enumerate(chars2_sorted):\n",
    "                        if ci == cj and not j in d and not i in d.values():\n",
    "                            d[j] = i\n",
    "                            continue\n",
    "                chars2_length = [len(c) for c in chars2_sorted]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2_sorted):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "            \n",
    "            if parameters['char_mode'] == 'CNN':\n",
    "\n",
    "                d = {}\n",
    "\n",
    "                ## Padding the each word to max word size of that sentence\n",
    "                chars2_length = [len(c) for c in chars2]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "\n",
    "            targets = torch.LongTensor(tags)\n",
    "\n",
    "            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
    "            if use_gpu:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "            else:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
    "            loss += neg_log_likelihood.data.item() / len(data['words'])\n",
    "            neg_log_likelihood.backward()\n",
    "\n",
    "            #we use gradient clipping to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            #Storing loss\n",
    "            if count % plot_every == 0:\n",
    "                loss /= plot_every\n",
    "                print(count, ': ', loss)\n",
    "                if losses == []:\n",
    "                    losses.append(loss)\n",
    "                losses.append(loss)\n",
    "                loss = 0.0\n",
    "\n",
    "            #Evaluating on Train, Test, Dev Sets\n",
    "            if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
    "                    count % (eval_every*4) == 0 and count < (eval_every * 20):\n",
    "                model.train(False)\n",
    "                best_train_F, new_train_F, _ = evaluating(model, train_data, best_train_F,\"Train\")\n",
    "                best_dev_F, new_dev_F, save = evaluating(model, dev_data, best_dev_F,\"Dev\")\n",
    "                if save:\n",
    "                    print(\"Saving Model to \", model_name)\n",
    "                    torch.save(model.state_dict(), model_name)\n",
    "                best_test_F, new_test_F, _ = evaluating(model, test_data, best_test_F,\"Test\")\n",
    "\n",
    "                all_F.append([new_train_F, new_dev_F, new_test_F])\n",
    "                model.train(True)\n",
    "\n",
    "            #Performing decay on the learning rate\n",
    "            if count % len(train_data) == 0:\n",
    "                adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n",
    "\n",
    "    print(time.time() - tr)\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "if not parameters['reload']:\n",
    "    #reload the best model saved from training\n",
    "    model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYSYFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/marcel/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marcel/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe0b17a2fb0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from torch import jit\n",
    "from torch import nn\n",
    "from syft.serde import protobuf\n",
    "import os\n",
    "from syft.execution.state import State\n",
    "from syft.execution.placeholder import PlaceHolder\n",
    "\n",
    "\n",
    "\n",
    "sy.make_hook(globals())\n",
    "# force protobuf serialization for tensors\n",
    "hook.local_worker.framework = None\n",
    "th.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_to_bin_pb(worker, obj, filename):\n",
    "    pb = protobuf.serde._bufferize(worker, obj)\n",
    "    bin = pb.SerializeToString()\n",
    "    print(\"Writing %s to %s/%s\" % (obj.__class__.__name__, os.getcwd(), filename))\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(bin)\n",
    "\n",
    "\n",
    "def set_model_params(module, params_list, start_param_idx=0):\n",
    "    \"\"\" Set params list into model recursively\n",
    "    \"\"\"\n",
    "    param_idx = start_param_idx\n",
    "\n",
    "    for name, param in module._parameters.items():\n",
    "        module._parameters[name] = params_list[param_idx]\n",
    "        param_idx += 1\n",
    "\n",
    "    for name, child in module._modules.items():\n",
    "        if child is not None:\n",
    "            param_idx += set_model_params(child, params_list, param_idx)\n",
    "\n",
    "    return param_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan()\n",
    "def training_plan(batch_size, lr, model_params):\n",
    "    # inject params into model\n",
    "    #set_model_params(model, model_params)\n",
    "\n",
    "    count = 0 #Counts the number of iterations\n",
    "    tr = time.time()\n",
    "    model.train(True)\n",
    "    for epoch in range(1,number_of_epochs):\n",
    "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "            count += 1\n",
    "            data = train_data[index]\n",
    "\n",
    "            ##gradient updates for each data entry\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = data['words']\n",
    "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "            tags = data['tags']\n",
    "            chars2 = data['chars']\n",
    "            \n",
    "            if parameters['char_mode'] == 'LSTM':\n",
    "                chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "                d = {}\n",
    "                for i, ci in enumerate(chars2):\n",
    "                    for j, cj in enumerate(chars2_sorted):\n",
    "                        if ci == cj and not j in d and not i in d.values():\n",
    "                            d[j] = i\n",
    "                            continue\n",
    "                chars2_length = [len(c) for c in chars2_sorted]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2_sorted):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "            \n",
    "            if parameters['char_mode'] == 'CNN':\n",
    "\n",
    "                d = {}\n",
    "\n",
    "                ## Padding the each word to max word size of that sentence\n",
    "                chars2_length = [len(c) for c in chars2]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "\n",
    "            targets = torch.LongTensor(tags)\n",
    "\n",
    "            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
    "            if use_gpu:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "            else:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
    "            loss += neg_log_likelihood.data.item() / len(data['words'])\n",
    "            neg_log_likelihood.backward()\n",
    "\n",
    "            #we use gradient clipping to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            #Storing loss\n",
    "            if count % plot_every == 0:\n",
    "                loss /= plot_every\n",
    "                print(count, ': ', loss)\n",
    "                if losses == []:\n",
    "                    losses.append(loss)\n",
    "                losses.append(loss)\n",
    "                loss = 0.0\n",
    "\n",
    "            #Evaluating on Train, Test, Dev Sets\n",
    "            if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
    "                    count % (eval_every*4) == 0 and count < (eval_every * 20):\n",
    "                model.train(False)\n",
    "                best_train_F, new_train_F, _ = evaluating(model, train_data, best_train_F,\"Train\")\n",
    "                best_dev_F, new_dev_F, save = evaluating(model, dev_data, best_dev_F,\"Dev\")\n",
    "                if save:\n",
    "                    print(\"Saving Model to \", model_name)\n",
    "                    torch.save(model.state_dict(), model_name)\n",
    "                best_test_F, new_test_F, _ = evaluating(model, test_data, best_test_F,\"Test\")\n",
    "\n",
    "                all_F.append([new_train_F, new_dev_F, new_test_F])\n",
    "                model.train(True)\n",
    "\n",
    "            #Performing decay on the learning rate\n",
    "            if count % len(train_data) == 0:\n",
    "                adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n",
    "\n",
    "        print(time.time() - tr)\n",
    "        plt.plot(losses)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PlaceHolder[Id:90959024525]>../data/eng.train, PlaceHolder[Id:11876063404]>../data/eng.testa, PlaceHolder[Id:52215426457]>AutogradTensor>PlaceHolder[Id:31279874222]>Parameter containing:\n",
      "tensor([-9.7684e-02, -1.9035e-01, -1.7424e-01, -9.7548e-02,  6.6590e-02,\n",
      "        -5.5609e-02, -4.6714e-01,  2.0046e-02, -3.4165e-01,  2.1935e-01,\n",
      "        -3.1880e-01,  5.8849e-02,  3.6245e-02,  5.1916e-02, -1.5045e-01,\n",
      "        -2.9095e-01, -6.2016e-02, -1.9516e-01, -2.6502e-02, -3.0087e-01,\n",
      "         2.5273e-01, -3.3973e-02, -5.4771e-02, -6.9037e-02,  1.3471e-01,\n",
      "        -3.0035e-01, -1.0811e-01, -9.7404e-02, -2.0780e-02, -3.4107e-01,\n",
      "        -2.7319e-01, -2.9391e-01, -5.6123e-02, -2.4450e-01, -2.4024e-01,\n",
      "        -1.7200e-01,  1.2293e-04, -1.8925e-01, -2.0725e-01, -1.2898e-01,\n",
      "        -2.0673e-01, -3.7136e-01, -4.6411e-01, -1.6745e-01, -1.0909e-01,\n",
      "        -3.7188e-01, -4.9178e-01, -1.4460e-01,  1.2909e-01, -2.6426e-01,\n",
      "        -6.3116e-01, -5.6399e-01,  3.3961e-03, -5.7922e-01, -1.9177e-01,\n",
      "         1.3243e-01,  1.9788e-02, -7.8850e-02, -1.9224e-01,  3.4999e-02,\n",
      "        -2.6429e-01,  5.1717e-02, -2.7240e-01, -1.8205e-01, -1.6320e-01,\n",
      "         8.3001e-02,  4.9224e-02, -1.3526e-01, -6.0816e-02, -2.3948e-01,\n",
      "         1.6153e-02, -6.6850e-02, -1.1093e-02, -7.8767e-02,  1.7077e-01,\n",
      "        -1.0369e-01, -1.6557e-01, -1.8078e-01,  1.1637e-01, -3.7464e-02,\n",
      "        -3.1939e-02,  4.3010e-02, -2.8061e-01, -1.2236e-01, -3.3591e-01,\n",
      "         5.3721e-03, -4.0011e-02,  1.5059e-02, -3.6432e-01, -1.4011e-01,\n",
      "        -1.6347e-01, -1.2084e-01,  4.4030e-02,  1.0505e-01, -6.2153e-02,\n",
      "         3.4240e-02, -1.7811e-01,  1.1578e-02, -1.9444e-01,  8.8057e-02,\n",
      "        -2.0736e-01, -1.0122e-01, -8.7484e-02,  2.6308e-03, -2.8240e-01,\n",
      "        -5.6065e-02,  1.6202e-01, -2.8625e-01, -1.3773e-01, -4.5850e-01,\n",
      "        -1.8073e-01, -1.9471e-01, -9.6289e-02, -2.3539e-01, -8.8841e-02,\n",
      "        -4.9664e-03, -2.4626e-01, -3.2135e-01, -3.1180e-01, -4.0575e-01,\n",
      "         9.9702e-02,  6.3277e-02, -1.7696e-04,  1.6959e-01, -7.1660e-01,\n",
      "        -4.2106e-01,  4.1162e-02, -8.7729e-02, -4.9849e-01, -2.5781e-01,\n",
      "         5.7233e-02,  2.3700e-01, -2.0468e-01, -1.8604e-01, -7.7262e-02,\n",
      "        -4.7082e-02, -3.3199e-01, -2.6907e-03,  1.0010e-01,  7.8234e-02,\n",
      "        -8.9790e-02, -2.8957e-01,  5.9772e-02, -3.6687e-01, -1.9616e-03,\n",
      "        -2.1082e-01, -7.0088e-02, -2.6545e-01,  8.7921e-02, -1.6007e-01,\n",
      "        -4.2448e-02,  7.8112e-02,  3.7638e-01, -2.3472e-01, -4.9971e-02,\n",
      "        -2.4137e-01,  6.5733e-02, -8.6810e-02, -1.6566e-01, -9.2850e-02,\n",
      "        -1.9562e-01, -1.5094e-01, -5.7365e-02,  1.3224e-01, -5.5122e-01,\n",
      "        -2.2211e-02, -8.8627e-02, -8.4832e-02, -8.7210e-02, -9.2882e-02,\n",
      "        -4.8866e-02,  1.4268e-03, -1.9317e-01, -3.0268e-01,  2.8378e-02,\n",
      "        -3.7397e-01,  9.4343e-02, -1.0817e-01, -2.6380e-01, -2.2096e-01,\n",
      "        -3.0265e-01, -3.0556e-01, -1.8835e-01, -5.8264e-01, -9.4726e-02,\n",
      "         5.4391e-02, -2.5494e-01, -3.6798e-01, -3.2093e-01, -2.8276e-01,\n",
      "        -2.2803e-01, -2.5556e-01,  4.0431e-03, -1.7836e-01, -1.1373e-01,\n",
      "        -1.6105e-01, -2.2606e-04,  1.7048e-01, -9.4321e-03, -2.1131e-01,\n",
      "         9.1955e-01,  8.7462e-01,  8.3969e-01,  6.5406e-01,  8.0815e-01,\n",
      "         8.0829e-01,  1.0158e+00,  7.8346e-01,  8.0755e-01,  8.4958e-01,\n",
      "         1.0410e+00,  5.4430e-01,  8.1938e-01,  8.1393e-01,  6.7920e-01,\n",
      "         8.9127e-01,  8.0804e-01,  8.7469e-01,  9.3887e-01,  9.6369e-01,\n",
      "         1.1810e+00,  9.9204e-01,  8.6734e-01,  6.0897e-01,  9.0408e-01,\n",
      "         8.5600e-01,  8.1072e-01,  7.5008e-01,  8.5193e-01,  5.2214e-01,\n",
      "         8.7683e-01,  1.0565e+00,  9.8366e-01,  1.0229e+00,  9.3139e-01,\n",
      "         7.6926e-01,  3.4102e-01,  7.5753e-01,  7.0910e-01,  7.6555e-01,\n",
      "         7.4417e-01,  8.4481e-01,  9.3494e-01,  8.7219e-01,  7.5489e-01,\n",
      "         5.5206e-01,  8.3939e-01,  1.1207e+00,  5.2566e-01,  9.6002e-01,\n",
      "         9.3136e-01,  8.3750e-01,  8.5060e-01,  6.8500e-01,  8.3125e-01,\n",
      "         8.9896e-01,  6.7541e-01,  8.4103e-01,  9.4982e-01,  8.8277e-01,\n",
      "         1.1154e+00,  7.6015e-01,  6.9901e-01,  8.4076e-01,  9.8255e-01,\n",
      "         7.0692e-01,  8.1235e-01,  8.9868e-01,  5.5886e-01,  9.8942e-01,\n",
      "         7.7100e-01,  7.6458e-01,  7.2202e-01,  9.6485e-01,  8.8944e-01,\n",
      "         6.5253e-01,  9.5350e-01,  9.8521e-01,  9.4923e-01,  8.6144e-01,\n",
      "         1.0444e+00,  6.7265e-01,  7.8438e-01,  3.2153e-01,  6.1909e-01,\n",
      "         8.9529e-01,  8.6087e-01,  1.0208e+00,  9.4112e-01,  7.8444e-01,\n",
      "         1.0528e+00,  7.7176e-01,  9.6572e-01,  9.1595e-01,  8.5990e-01,\n",
      "         6.8522e-01,  7.8266e-01,  7.2438e-01,  1.0206e+00,  6.7312e-01,\n",
      "         8.3408e-01,  9.4710e-01,  9.5669e-01,  9.8791e-01,  9.1094e-01,\n",
      "         3.9980e-01,  9.2343e-01,  8.7671e-01,  7.7845e-01,  1.0315e+00,\n",
      "         9.3967e-01,  7.8909e-01,  5.7278e-01,  9.4527e-01,  7.0400e-01,\n",
      "         5.5635e-01,  1.0201e+00,  5.1228e-01,  7.1342e-01,  6.5353e-01,\n",
      "         8.8543e-01,  6.1170e-01,  8.9345e-01,  4.0607e-01,  6.4561e-01,\n",
      "         4.3239e-01,  7.7684e-01,  5.8749e-01,  7.4182e-01,  9.8855e-01,\n",
      "         8.7232e-01,  9.2578e-01,  6.5836e-01,  9.2243e-01,  7.6795e-01,\n",
      "         6.9793e-01,  1.0403e+00,  7.4481e-01,  4.6516e-01,  7.3994e-01,\n",
      "         7.8569e-01,  7.3919e-01,  7.9429e-01,  5.6388e-01,  8.4758e-01,\n",
      "         9.6997e-01,  9.1017e-01,  9.0215e-01,  4.9265e-01,  7.7935e-01,\n",
      "         7.2732e-02,  8.1613e-01,  1.0784e+00,  7.4565e-01,  8.2109e-01,\n",
      "         8.3439e-01,  9.3212e-01,  8.2079e-01,  1.1432e+00,  9.8848e-01,\n",
      "         6.1868e-01,  6.7718e-01,  8.3478e-01,  1.0196e+00,  8.9025e-01,\n",
      "         9.9054e-01,  6.1598e-01,  1.0058e+00,  9.0388e-01,  8.7458e-01,\n",
      "         1.2986e+00,  8.0378e-01,  9.7514e-01,  9.7813e-01,  8.2485e-01,\n",
      "         7.4854e-01,  6.1338e-01,  5.9496e-01,  7.8757e-01,  8.5395e-01,\n",
      "         8.6280e-01,  7.5089e-01,  9.0887e-01,  1.1476e+00,  1.0873e+00,\n",
      "         7.2777e-01,  7.4729e-01,  5.7484e-01,  1.0145e+00,  8.9294e-01,\n",
      "         7.7855e-01,  8.6505e-01,  7.2196e-01,  9.5526e-01,  8.7862e-01,\n",
      "         1.8137e-01,  7.4810e-01,  7.4602e-01,  3.0640e-01,  6.8740e-01,\n",
      "         1.4278e-03,  9.8463e-02,  5.4639e-01,  6.1884e-02, -2.7154e-01,\n",
      "         2.4213e-01,  2.3587e-01, -4.5867e-02, -2.8904e-01,  1.5456e-01,\n",
      "         3.6020e-01, -2.4376e-01, -1.0357e-01, -7.0327e-01, -1.7468e-01,\n",
      "        -7.5030e-02,  1.9776e-01, -6.9596e-01,  4.2823e-01, -2.7109e-01,\n",
      "         1.8006e-01,  8.0833e-02,  1.6613e-01,  4.3513e-01,  1.2612e-01,\n",
      "        -1.2823e-02, -5.1390e-01,  1.6000e-01, -1.9023e-01, -4.5535e-02,\n",
      "         2.7352e-01,  3.0720e-01, -3.0515e-01, -3.9164e-01, -2.0079e-01,\n",
      "        -5.7885e-01, -2.8800e-01, -1.8181e-01, -4.7485e-01, -5.6045e-01,\n",
      "         3.7940e-01,  3.8337e-01, -3.0136e-01, -6.9791e-02,  1.7122e-01,\n",
      "        -3.2325e-02,  6.8370e-02,  2.6556e-01,  1.2171e-01, -5.6097e-01,\n",
      "         4.7462e-01, -2.0043e-01, -1.9352e-01,  3.8693e-01,  2.0798e-01,\n",
      "        -1.3111e-01,  2.4645e-01,  4.8275e-01, -3.1942e-01, -3.1366e-01,\n",
      "         3.1959e-01, -8.2945e-01,  6.0894e-02, -2.6709e-01, -7.0336e-02,\n",
      "         8.1490e-02, -6.5751e-02,  3.5728e-01, -5.6530e-03,  4.1118e-01,\n",
      "        -2.3882e-01, -1.8672e-01, -2.1833e-01, -5.1440e-02, -8.1007e-03,\n",
      "         4.8670e-01, -2.8268e-02,  3.0911e-01,  1.5644e-01,  1.0762e-01,\n",
      "         3.9918e-01,  9.9574e-02,  2.7025e-02,  7.3850e-01, -8.5950e-01,\n",
      "        -6.1311e-01, -9.8304e-02,  2.1872e-01, -8.4889e-02,  3.8473e-01,\n",
      "         1.9260e-01,  6.5761e-01,  4.0395e-01, -7.1151e-01, -2.8296e-01,\n",
      "        -2.8526e-01, -2.7861e-01, -3.7150e-01,  3.1986e-01,  6.0759e-01,\n",
      "         4.5411e-01,  1.3383e-01,  1.9350e-01,  3.6184e-02,  3.3120e-01,\n",
      "         3.5278e-01, -3.9608e-01, -5.2399e-01, -1.0331e-01, -2.6657e-01,\n",
      "         2.1334e-01, -1.8702e-01,  4.9889e-02,  1.6232e-01, -3.5800e-01,\n",
      "         3.4583e-02,  4.3510e-02, -2.4906e-01, -2.7981e-01,  1.3931e-01,\n",
      "        -5.5007e-01, -4.6789e-01, -1.9660e-01, -2.6802e-01,  3.9671e-02,\n",
      "         2.8194e-01,  6.5404e-01, -1.4117e-01, -3.1999e-01, -1.2291e-02,\n",
      "         2.2953e-01,  1.8556e-01,  5.0388e-01,  6.1848e-01, -4.9514e-01,\n",
      "        -3.6202e-01, -1.0448e-01,  4.5127e-01, -3.0947e-01, -1.7481e-01,\n",
      "         1.6442e-02,  4.1746e-01, -6.6682e-01,  5.0309e-01, -2.6276e-01,\n",
      "        -5.2682e-01, -1.2662e-01, -7.1933e-01,  2.6989e-01,  5.7127e-01,\n",
      "        -4.9057e-01,  5.7128e-03,  6.5044e-01,  1.5832e-01, -3.9113e-01,\n",
      "         9.3375e-02, -5.4186e-01, -4.3874e-02, -7.4900e-02, -4.2908e-02,\n",
      "         7.0391e-02,  1.0723e-01, -5.4150e-02, -2.3840e-01, -6.0680e-02,\n",
      "        -7.2016e-02, -1.1598e-01, -3.0908e-01, -1.6452e-01, -1.1974e-01,\n",
      "         1.8345e-01,  9.4208e-02, -2.9529e-01,  2.0711e-02, -5.7070e-01,\n",
      "         2.4096e-01,  8.5671e-02,  4.8009e-01, -1.2553e-01,  3.0733e-01,\n",
      "         3.7943e-01, -2.8814e-02, -2.0238e-01, -1.0263e-01, -4.4182e-01,\n",
      "         7.8051e-01,  6.2551e-01,  2.7812e-02, -1.6324e-01,  2.3333e-02,\n",
      "         1.9665e-01, -1.3114e-01, -1.6127e-02, -7.5951e-02, -7.1420e-01,\n",
      "        -2.0077e-02,  1.9682e-01,  3.1428e-01, -1.1866e-02,  3.3046e-01,\n",
      "        -6.0140e-01, -3.0346e-01, -4.1196e-01, -2.5876e-01, -1.2890e-01,\n",
      "        -5.0165e-01, -3.4487e-01, -2.5679e-01, -2.5510e-01, -3.1141e-01,\n",
      "        -9.6256e-01, -1.6112e-01,  2.7624e-02, -1.0790e-01, -3.1275e-01,\n",
      "        -4.4154e-01, -1.7567e-01, -1.5342e-01, -3.9169e-01, -6.7650e-01,\n",
      "        -5.0304e-01, -1.9964e-02, -2.5868e-01, -3.1179e-01, -2.0838e-01,\n",
      "        -5.8770e-01, -3.9910e-01, -3.5618e-01, -4.9050e-01, -2.9422e-01,\n",
      "        -2.4247e-01, -2.8429e-01, -3.6677e-02, -2.2885e-01, -5.6926e-01,\n",
      "        -2.9966e-01, -1.2426e-01, -3.0807e-01, -2.8464e-01, -4.3549e-01,\n",
      "        -6.9063e-01, -3.4259e-01, -5.6359e-01, -2.4922e-01, -5.0297e-01,\n",
      "        -4.3372e-01, -3.6401e-01, -1.2139e-01, -1.3983e-01, -2.0886e-01,\n",
      "        -4.1179e-01, -4.8034e-01, -3.4580e-01, -5.1785e-01, -1.5049e-01,\n",
      "        -1.6592e-01, -3.1400e-01, -9.5982e-01, -1.2847e-01, -2.0299e-01,\n",
      "        -7.6757e-01, -2.6631e-01, -2.3318e-01, -1.2092e-01, -1.5641e-01,\n",
      "        -6.7965e-01, -3.4488e-01, -5.3538e-01, -7.4866e-02, -2.7088e-01,\n",
      "        -3.8774e-01, -4.4732e-01, -2.4539e-01, -6.5708e-01, -3.7782e-01,\n",
      "        -3.6176e-01, -3.5715e-02, -8.6624e-02, -1.0403e-01, -3.0789e-01,\n",
      "        -7.6411e-01, -4.6061e-01, -3.6659e-01, -4.1470e-01, -5.4956e-01,\n",
      "        -6.6727e-02, -3.1873e-01, -2.6416e-01, -3.8453e-01, -1.5477e-01,\n",
      "        -5.5524e-01, -2.5042e-01, -5.6192e-01, -5.6467e-01, -3.2564e-01,\n",
      "        -1.9211e-01, -4.3016e-01, -5.0237e-01, -3.6818e-01, -4.0939e-01,\n",
      "        -3.5111e-01, -2.9284e-01, -4.0926e-01, -1.7565e-01, -6.1417e-01,\n",
      "        -2.8147e-02, -2.2463e-01, -1.4299e-01, -3.6546e-01, -5.5246e-01,\n",
      "        -2.2649e-01, -5.4433e-01, -6.5060e-01, -6.8854e-01, -1.6636e-01,\n",
      "        -1.0364e-01, -7.8424e-01, -4.5118e-01, -4.2304e-01, -6.6483e-01,\n",
      "        -2.6467e-01, -6.1959e-02, -2.6587e-01, -2.1531e-01, -4.8208e-01,\n",
      "        -5.9461e-01, -7.5356e-01, -3.1838e-01, -4.3683e-01, -3.5605e-01,\n",
      "        -4.2825e-01, -1.6073e-01, -3.9383e-01, -2.3532e-01, -4.8675e-01,\n",
      "        -4.0493e-01, -3.4288e-01, -3.1979e-01, -1.9949e-01, -4.0583e-01,\n",
      "        -4.6598e-01, -3.7904e-01, -8.3139e-01, -7.3022e-01, -2.8643e-01,\n",
      "        -3.2406e-01, -2.7547e-01, -5.1462e-01, -3.4213e-01, -3.7178e-01,\n",
      "        -4.9805e-01, -1.0484e-01, -1.0608e+00, -3.8095e-01, -3.9614e-01,\n",
      "        -4.4622e-01, -3.3181e-01, -5.6958e-01, -5.4431e-01, -1.5188e-01,\n",
      "        -3.1286e-01, -4.0539e-01, -7.5996e-01, -2.4477e-01, -7.9193e-01,\n",
      "        -4.4644e-02, -2.5442e-01, -4.9655e-01, -3.0449e-01, -2.5090e-01,\n",
      "        -8.9006e-02, -3.3069e-01, -5.9929e-01, -3.3034e-01, -4.4092e-01,\n",
      "        -4.0550e-01, -1.8025e-01, -3.6270e-01, -3.3770e-01, -4.7527e-01,\n",
      "        -3.6574e-01, -5.4066e-01, -3.4407e-01, -8.4674e-01, -7.0090e-01,\n",
      "        -5.8012e-01, -4.0686e-01, -4.0257e-01, -4.4235e-01, -2.7991e-01,\n",
      "        -7.5184e-01, -5.8519e-01, -2.3481e-01, -4.4350e-01, -2.2110e-01,\n",
      "        -1.7259e-01, -2.2409e-01,  1.8470e-01, -1.8290e-01, -6.1163e-01],\n",
      "       requires_grad=True), PlaceHolder[Id:95094203762]>AutogradTensor>PlaceHolder[Id:17986392809]>Parameter containing:\n",
      "tensor([-9.7684e-02, -1.9035e-01, -1.7424e-01, -9.7548e-02,  6.6590e-02,\n",
      "        -5.5609e-02, -4.6714e-01,  2.0046e-02, -3.4165e-01,  2.1935e-01,\n",
      "        -3.1880e-01,  5.8849e-02,  3.6245e-02,  5.1916e-02, -1.5045e-01,\n",
      "        -2.9095e-01, -6.2016e-02, -1.9516e-01, -2.6502e-02, -3.0087e-01,\n",
      "         2.5273e-01, -3.3973e-02, -5.4771e-02, -6.9037e-02,  1.3471e-01,\n",
      "        -3.0035e-01, -1.0811e-01, -9.7404e-02, -2.0780e-02, -3.4107e-01,\n",
      "        -2.7319e-01, -2.9391e-01, -5.6123e-02, -2.4450e-01, -2.4024e-01,\n",
      "        -1.7200e-01,  1.2293e-04, -1.8925e-01, -2.0725e-01, -1.2898e-01,\n",
      "        -2.0673e-01, -3.7136e-01, -4.6411e-01, -1.6745e-01, -1.0909e-01,\n",
      "        -3.7188e-01, -4.9178e-01, -1.4460e-01,  1.2909e-01, -2.6426e-01,\n",
      "        -6.3116e-01, -5.6399e-01,  3.3961e-03, -5.7922e-01, -1.9177e-01,\n",
      "         1.3243e-01,  1.9788e-02, -7.8850e-02, -1.9224e-01,  3.4999e-02,\n",
      "        -2.6429e-01,  5.1717e-02, -2.7240e-01, -1.8205e-01, -1.6320e-01,\n",
      "         8.3001e-02,  4.9224e-02, -1.3526e-01, -6.0816e-02, -2.3948e-01,\n",
      "         1.6153e-02, -6.6850e-02, -1.1093e-02, -7.8767e-02,  1.7077e-01,\n",
      "        -1.0369e-01, -1.6557e-01, -1.8078e-01,  1.1637e-01, -3.7464e-02,\n",
      "        -3.1939e-02,  4.3010e-02, -2.8061e-01, -1.2236e-01, -3.3591e-01,\n",
      "         5.3721e-03, -4.0011e-02,  1.5059e-02, -3.6432e-01, -1.4011e-01,\n",
      "        -1.6347e-01, -1.2084e-01,  4.4030e-02,  1.0505e-01, -6.2153e-02,\n",
      "         3.4240e-02, -1.7811e-01,  1.1578e-02, -1.9444e-01,  8.8057e-02,\n",
      "        -2.0736e-01, -1.0122e-01, -8.7484e-02,  2.6308e-03, -2.8240e-01,\n",
      "        -5.6065e-02,  1.6202e-01, -2.8625e-01, -1.3773e-01, -4.5850e-01,\n",
      "        -1.8073e-01, -1.9471e-01, -9.6289e-02, -2.3539e-01, -8.8841e-02,\n",
      "        -4.9664e-03, -2.4626e-01, -3.2135e-01, -3.1180e-01, -4.0575e-01,\n",
      "         9.9702e-02,  6.3277e-02, -1.7696e-04,  1.6959e-01, -7.1660e-01,\n",
      "        -4.2106e-01,  4.1162e-02, -8.7729e-02, -4.9849e-01, -2.5781e-01,\n",
      "         5.7233e-02,  2.3700e-01, -2.0468e-01, -1.8604e-01, -7.7262e-02,\n",
      "        -4.7082e-02, -3.3199e-01, -2.6907e-03,  1.0010e-01,  7.8234e-02,\n",
      "        -8.9790e-02, -2.8957e-01,  5.9772e-02, -3.6687e-01, -1.9616e-03,\n",
      "        -2.1082e-01, -7.0088e-02, -2.6545e-01,  8.7921e-02, -1.6007e-01,\n",
      "        -4.2448e-02,  7.8112e-02,  3.7638e-01, -2.3472e-01, -4.9971e-02,\n",
      "        -2.4137e-01,  6.5733e-02, -8.6810e-02, -1.6566e-01, -9.2850e-02,\n",
      "        -1.9562e-01, -1.5094e-01, -5.7365e-02,  1.3224e-01, -5.5122e-01,\n",
      "        -2.2211e-02, -8.8627e-02, -8.4832e-02, -8.7210e-02, -9.2882e-02,\n",
      "        -4.8866e-02,  1.4268e-03, -1.9317e-01, -3.0268e-01,  2.8378e-02,\n",
      "        -3.7397e-01,  9.4343e-02, -1.0817e-01, -2.6380e-01, -2.2096e-01,\n",
      "        -3.0265e-01, -3.0556e-01, -1.8835e-01, -5.8264e-01, -9.4726e-02,\n",
      "         5.4391e-02, -2.5494e-01, -3.6798e-01, -3.2093e-01, -2.8276e-01,\n",
      "        -2.2803e-01, -2.5556e-01,  4.0431e-03, -1.7836e-01, -1.1373e-01,\n",
      "        -1.6105e-01, -2.2606e-04,  1.7048e-01, -9.4321e-03, -2.1131e-01,\n",
      "         9.1955e-01,  8.7462e-01,  8.3969e-01,  6.5406e-01,  8.0815e-01,\n",
      "         8.0829e-01,  1.0158e+00,  7.8346e-01,  8.0755e-01,  8.4958e-01,\n",
      "         1.0410e+00,  5.4430e-01,  8.1938e-01,  8.1393e-01,  6.7920e-01,\n",
      "         8.9127e-01,  8.0804e-01,  8.7469e-01,  9.3887e-01,  9.6369e-01,\n",
      "         1.1810e+00,  9.9204e-01,  8.6734e-01,  6.0897e-01,  9.0408e-01,\n",
      "         8.5600e-01,  8.1072e-01,  7.5008e-01,  8.5193e-01,  5.2214e-01,\n",
      "         8.7683e-01,  1.0565e+00,  9.8366e-01,  1.0229e+00,  9.3139e-01,\n",
      "         7.6926e-01,  3.4102e-01,  7.5753e-01,  7.0910e-01,  7.6555e-01,\n",
      "         7.4417e-01,  8.4481e-01,  9.3494e-01,  8.7219e-01,  7.5489e-01,\n",
      "         5.5206e-01,  8.3939e-01,  1.1207e+00,  5.2566e-01,  9.6002e-01,\n",
      "         9.3136e-01,  8.3750e-01,  8.5060e-01,  6.8500e-01,  8.3125e-01,\n",
      "         8.9896e-01,  6.7541e-01,  8.4103e-01,  9.4982e-01,  8.8277e-01,\n",
      "         1.1154e+00,  7.6015e-01,  6.9901e-01,  8.4076e-01,  9.8255e-01,\n",
      "         7.0692e-01,  8.1235e-01,  8.9868e-01,  5.5886e-01,  9.8942e-01,\n",
      "         7.7100e-01,  7.6458e-01,  7.2202e-01,  9.6485e-01,  8.8944e-01,\n",
      "         6.5253e-01,  9.5350e-01,  9.8521e-01,  9.4923e-01,  8.6144e-01,\n",
      "         1.0444e+00,  6.7265e-01,  7.8438e-01,  3.2153e-01,  6.1909e-01,\n",
      "         8.9529e-01,  8.6087e-01,  1.0208e+00,  9.4112e-01,  7.8444e-01,\n",
      "         1.0528e+00,  7.7176e-01,  9.6572e-01,  9.1595e-01,  8.5990e-01,\n",
      "         6.8522e-01,  7.8266e-01,  7.2438e-01,  1.0206e+00,  6.7312e-01,\n",
      "         8.3408e-01,  9.4710e-01,  9.5669e-01,  9.8791e-01,  9.1094e-01,\n",
      "         3.9980e-01,  9.2343e-01,  8.7671e-01,  7.7845e-01,  1.0315e+00,\n",
      "         9.3967e-01,  7.8909e-01,  5.7278e-01,  9.4527e-01,  7.0400e-01,\n",
      "         5.5635e-01,  1.0201e+00,  5.1228e-01,  7.1342e-01,  6.5353e-01,\n",
      "         8.8543e-01,  6.1170e-01,  8.9345e-01,  4.0607e-01,  6.4561e-01,\n",
      "         4.3239e-01,  7.7684e-01,  5.8749e-01,  7.4182e-01,  9.8855e-01,\n",
      "         8.7232e-01,  9.2578e-01,  6.5836e-01,  9.2243e-01,  7.6795e-01,\n",
      "         6.9793e-01,  1.0403e+00,  7.4481e-01,  4.6516e-01,  7.3994e-01,\n",
      "         7.8569e-01,  7.3919e-01,  7.9429e-01,  5.6388e-01,  8.4758e-01,\n",
      "         9.6997e-01,  9.1017e-01,  9.0215e-01,  4.9265e-01,  7.7935e-01,\n",
      "         7.2732e-02,  8.1613e-01,  1.0784e+00,  7.4565e-01,  8.2109e-01,\n",
      "         8.3439e-01,  9.3212e-01,  8.2079e-01,  1.1432e+00,  9.8848e-01,\n",
      "         6.1868e-01,  6.7718e-01,  8.3478e-01,  1.0196e+00,  8.9025e-01,\n",
      "         9.9054e-01,  6.1598e-01,  1.0058e+00,  9.0388e-01,  8.7458e-01,\n",
      "         1.2986e+00,  8.0378e-01,  9.7514e-01,  9.7813e-01,  8.2485e-01,\n",
      "         7.4854e-01,  6.1338e-01,  5.9496e-01,  7.8757e-01,  8.5395e-01,\n",
      "         8.6280e-01,  7.5089e-01,  9.0887e-01,  1.1476e+00,  1.0873e+00,\n",
      "         7.2777e-01,  7.4729e-01,  5.7484e-01,  1.0145e+00,  8.9294e-01,\n",
      "         7.7855e-01,  8.6505e-01,  7.2196e-01,  9.5526e-01,  8.7862e-01,\n",
      "         1.8137e-01,  7.4810e-01,  7.4602e-01,  3.0640e-01,  6.8740e-01,\n",
      "         1.4278e-03,  9.8463e-02,  5.4639e-01,  6.1884e-02, -2.7154e-01,\n",
      "         2.4213e-01,  2.3587e-01, -4.5867e-02, -2.8904e-01,  1.5456e-01,\n",
      "         3.6020e-01, -2.4376e-01, -1.0357e-01, -7.0327e-01, -1.7468e-01,\n",
      "        -7.5030e-02,  1.9776e-01, -6.9596e-01,  4.2823e-01, -2.7109e-01,\n",
      "         1.8006e-01,  8.0833e-02,  1.6613e-01,  4.3513e-01,  1.2612e-01,\n",
      "        -1.2823e-02, -5.1390e-01,  1.6000e-01, -1.9023e-01, -4.5535e-02,\n",
      "         2.7352e-01,  3.0720e-01, -3.0515e-01, -3.9164e-01, -2.0079e-01,\n",
      "        -5.7885e-01, -2.8800e-01, -1.8181e-01, -4.7485e-01, -5.6045e-01,\n",
      "         3.7940e-01,  3.8337e-01, -3.0136e-01, -6.9791e-02,  1.7122e-01,\n",
      "        -3.2325e-02,  6.8370e-02,  2.6556e-01,  1.2171e-01, -5.6097e-01,\n",
      "         4.7462e-01, -2.0043e-01, -1.9352e-01,  3.8693e-01,  2.0798e-01,\n",
      "        -1.3111e-01,  2.4645e-01,  4.8275e-01, -3.1942e-01, -3.1366e-01,\n",
      "         3.1959e-01, -8.2945e-01,  6.0894e-02, -2.6709e-01, -7.0336e-02,\n",
      "         8.1490e-02, -6.5751e-02,  3.5728e-01, -5.6530e-03,  4.1118e-01,\n",
      "        -2.3882e-01, -1.8672e-01, -2.1833e-01, -5.1440e-02, -8.1007e-03,\n",
      "         4.8670e-01, -2.8268e-02,  3.0911e-01,  1.5644e-01,  1.0762e-01,\n",
      "         3.9918e-01,  9.9574e-02,  2.7025e-02,  7.3850e-01, -8.5950e-01,\n",
      "        -6.1311e-01, -9.8304e-02,  2.1872e-01, -8.4889e-02,  3.8473e-01,\n",
      "         1.9260e-01,  6.5761e-01,  4.0395e-01, -7.1151e-01, -2.8296e-01,\n",
      "        -2.8526e-01, -2.7861e-01, -3.7150e-01,  3.1986e-01,  6.0759e-01,\n",
      "         4.5411e-01,  1.3383e-01,  1.9350e-01,  3.6184e-02,  3.3120e-01,\n",
      "         3.5278e-01, -3.9608e-01, -5.2399e-01, -1.0331e-01, -2.6657e-01,\n",
      "         2.1334e-01, -1.8702e-01,  4.9889e-02,  1.6232e-01, -3.5800e-01,\n",
      "         3.4583e-02,  4.3510e-02, -2.4906e-01, -2.7981e-01,  1.3931e-01,\n",
      "        -5.5007e-01, -4.6789e-01, -1.9660e-01, -2.6802e-01,  3.9671e-02,\n",
      "         2.8194e-01,  6.5404e-01, -1.4117e-01, -3.1999e-01, -1.2291e-02,\n",
      "         2.2953e-01,  1.8556e-01,  5.0388e-01,  6.1848e-01, -4.9514e-01,\n",
      "        -3.6202e-01, -1.0448e-01,  4.5127e-01, -3.0947e-01, -1.7481e-01,\n",
      "         1.6442e-02,  4.1746e-01, -6.6682e-01,  5.0309e-01, -2.6276e-01,\n",
      "        -5.2682e-01, -1.2662e-01, -7.1933e-01,  2.6989e-01,  5.7127e-01,\n",
      "        -4.9057e-01,  5.7128e-03,  6.5044e-01,  1.5832e-01, -3.9113e-01,\n",
      "         9.3375e-02, -5.4186e-01, -4.3874e-02, -7.4900e-02, -4.2908e-02,\n",
      "         7.0391e-02,  1.0723e-01, -5.4150e-02, -2.3840e-01, -6.0680e-02,\n",
      "        -7.2016e-02, -1.1598e-01, -3.0908e-01, -1.6452e-01, -1.1974e-01,\n",
      "         1.8345e-01,  9.4208e-02, -2.9529e-01,  2.0711e-02, -5.7070e-01,\n",
      "         2.4096e-01,  8.5671e-02,  4.8009e-01, -1.2553e-01,  3.0733e-01,\n",
      "         3.7943e-01, -2.8814e-02, -2.0238e-01, -1.0263e-01, -4.4182e-01,\n",
      "         7.8051e-01,  6.2551e-01,  2.7812e-02, -1.6324e-01,  2.3333e-02,\n",
      "         1.9665e-01, -1.3114e-01, -1.6127e-02, -7.5951e-02, -7.1420e-01,\n",
      "        -2.0077e-02,  1.9682e-01,  3.1428e-01, -1.1866e-02,  3.3046e-01,\n",
      "        -6.0140e-01, -3.0346e-01, -4.1196e-01, -2.5876e-01, -1.2890e-01,\n",
      "        -5.0165e-01, -3.4487e-01, -2.5679e-01, -2.5510e-01, -3.1141e-01,\n",
      "        -9.6256e-01, -1.6112e-01,  2.7624e-02, -1.0790e-01, -3.1275e-01,\n",
      "        -4.4154e-01, -1.7567e-01, -1.5342e-01, -3.9169e-01, -6.7650e-01,\n",
      "        -5.0304e-01, -1.9964e-02, -2.5868e-01, -3.1179e-01, -2.0838e-01,\n",
      "        -5.8770e-01, -3.9910e-01, -3.5618e-01, -4.9050e-01, -2.9422e-01,\n",
      "        -2.4247e-01, -2.8429e-01, -3.6677e-02, -2.2885e-01, -5.6926e-01,\n",
      "        -2.9966e-01, -1.2426e-01, -3.0807e-01, -2.8464e-01, -4.3549e-01,\n",
      "        -6.9063e-01, -3.4259e-01, -5.6359e-01, -2.4922e-01, -5.0297e-01,\n",
      "        -4.3372e-01, -3.6401e-01, -1.2139e-01, -1.3983e-01, -2.0886e-01,\n",
      "        -4.1179e-01, -4.8034e-01, -3.4580e-01, -5.1785e-01, -1.5049e-01,\n",
      "        -1.6592e-01, -3.1400e-01, -9.5982e-01, -1.2847e-01, -2.0299e-01,\n",
      "        -7.6757e-01, -2.6631e-01, -2.3318e-01, -1.2092e-01, -1.5641e-01,\n",
      "        -6.7965e-01, -3.4488e-01, -5.3538e-01, -7.4866e-02, -2.7088e-01,\n",
      "        -3.8774e-01, -4.4732e-01, -2.4539e-01, -6.5708e-01, -3.7782e-01,\n",
      "        -3.6176e-01, -3.5715e-02, -8.6624e-02, -1.0403e-01, -3.0789e-01,\n",
      "        -7.6411e-01, -4.6061e-01, -3.6659e-01, -4.1470e-01, -5.4956e-01,\n",
      "        -6.6727e-02, -3.1873e-01, -2.6416e-01, -3.8453e-01, -1.5477e-01,\n",
      "        -5.5524e-01, -2.5042e-01, -5.6192e-01, -5.6467e-01, -3.2564e-01,\n",
      "        -1.9211e-01, -4.3016e-01, -5.0237e-01, -3.6818e-01, -4.0939e-01,\n",
      "        -3.5111e-01, -2.9284e-01, -4.0926e-01, -1.7565e-01, -6.1417e-01,\n",
      "        -2.8147e-02, -2.2463e-01, -1.4299e-01, -3.6546e-01, -5.5246e-01,\n",
      "        -2.2649e-01, -5.4433e-01, -6.5060e-01, -6.8854e-01, -1.6636e-01,\n",
      "        -1.0364e-01, -7.8424e-01, -4.5118e-01, -4.2304e-01, -6.6483e-01,\n",
      "        -2.6467e-01, -6.1959e-02, -2.6587e-01, -2.1531e-01, -4.8208e-01,\n",
      "        -5.9461e-01, -7.5356e-01, -3.1838e-01, -4.3683e-01, -3.5605e-01,\n",
      "        -4.2825e-01, -1.6073e-01, -3.9383e-01, -2.3532e-01, -4.8675e-01,\n",
      "        -4.0493e-01, -3.4288e-01, -3.1979e-01, -1.9949e-01, -4.0583e-01,\n",
      "        -4.6598e-01, -3.7904e-01, -8.3139e-01, -7.3022e-01, -2.8643e-01,\n",
      "        -3.2406e-01, -2.7547e-01, -5.1462e-01, -3.4213e-01, -3.7178e-01,\n",
      "        -4.9805e-01, -1.0484e-01, -1.0608e+00, -3.8095e-01, -3.9614e-01,\n",
      "        -4.4622e-01, -3.3181e-01, -5.6958e-01, -5.4431e-01, -1.5188e-01,\n",
      "        -3.1286e-01, -4.0539e-01, -7.5996e-01, -2.4477e-01, -7.9193e-01,\n",
      "        -4.4644e-02, -2.5442e-01, -4.9655e-01, -3.0449e-01, -2.5090e-01,\n",
      "        -8.9006e-02, -3.3069e-01, -5.9929e-01, -3.3034e-01, -4.4092e-01,\n",
      "        -4.0550e-01, -1.8025e-01, -3.6270e-01, -3.3770e-01, -4.7527e-01,\n",
      "        -3.6574e-01, -5.4066e-01, -3.4407e-01, -8.4674e-01, -7.0090e-01,\n",
      "        -5.8012e-01, -4.0686e-01, -4.0257e-01, -4.4235e-01, -2.7991e-01,\n",
      "        -7.5184e-01, -5.8519e-01, -2.3481e-01, -4.4350e-01, -2.2110e-01,\n",
      "        -1.7259e-01, -2.2409e-01,  1.8470e-01, -1.8290e-01, -6.1163e-01],\n",
      "       requires_grad=True), AutogradTensor>PlaceHolder[Id:29919034196]>Parameter containing:\n",
      "tensor([-9.7684e-02, -1.9035e-01, -1.7424e-01, -9.7548e-02,  6.6590e-02,\n",
      "        -5.5609e-02, -4.6714e-01,  2.0046e-02, -3.4165e-01,  2.1935e-01,\n",
      "        -3.1880e-01,  5.8849e-02,  3.6245e-02,  5.1916e-02, -1.5045e-01,\n",
      "        -2.9095e-01, -6.2016e-02, -1.9516e-01, -2.6502e-02, -3.0087e-01,\n",
      "         2.5273e-01, -3.3973e-02, -5.4771e-02, -6.9037e-02,  1.3471e-01,\n",
      "        -3.0035e-01, -1.0811e-01, -9.7404e-02, -2.0780e-02, -3.4107e-01,\n",
      "        -2.7319e-01, -2.9391e-01, -5.6123e-02, -2.4450e-01, -2.4024e-01,\n",
      "        -1.7200e-01,  1.2293e-04, -1.8925e-01, -2.0725e-01, -1.2898e-01,\n",
      "        -2.0673e-01, -3.7136e-01, -4.6411e-01, -1.6745e-01, -1.0909e-01,\n",
      "        -3.7188e-01, -4.9178e-01, -1.4460e-01,  1.2909e-01, -2.6426e-01,\n",
      "        -6.3116e-01, -5.6399e-01,  3.3961e-03, -5.7922e-01, -1.9177e-01,\n",
      "         1.3243e-01,  1.9788e-02, -7.8850e-02, -1.9224e-01,  3.4999e-02,\n",
      "        -2.6429e-01,  5.1717e-02, -2.7240e-01, -1.8205e-01, -1.6320e-01,\n",
      "         8.3001e-02,  4.9224e-02, -1.3526e-01, -6.0816e-02, -2.3948e-01,\n",
      "         1.6153e-02, -6.6850e-02, -1.1093e-02, -7.8767e-02,  1.7077e-01,\n",
      "        -1.0369e-01, -1.6557e-01, -1.8078e-01,  1.1637e-01, -3.7464e-02,\n",
      "        -3.1939e-02,  4.3010e-02, -2.8061e-01, -1.2236e-01, -3.3591e-01,\n",
      "         5.3721e-03, -4.0011e-02,  1.5059e-02, -3.6432e-01, -1.4011e-01,\n",
      "        -1.6347e-01, -1.2084e-01,  4.4030e-02,  1.0505e-01, -6.2153e-02,\n",
      "         3.4240e-02, -1.7811e-01,  1.1578e-02, -1.9444e-01,  8.8057e-02,\n",
      "        -2.0736e-01, -1.0122e-01, -8.7484e-02,  2.6308e-03, -2.8240e-01,\n",
      "        -5.6065e-02,  1.6202e-01, -2.8625e-01, -1.3773e-01, -4.5850e-01,\n",
      "        -1.8073e-01, -1.9471e-01, -9.6289e-02, -2.3539e-01, -8.8841e-02,\n",
      "        -4.9664e-03, -2.4626e-01, -3.2135e-01, -3.1180e-01, -4.0575e-01,\n",
      "         9.9702e-02,  6.3277e-02, -1.7696e-04,  1.6959e-01, -7.1660e-01,\n",
      "        -4.2106e-01,  4.1162e-02, -8.7729e-02, -4.9849e-01, -2.5781e-01,\n",
      "         5.7233e-02,  2.3700e-01, -2.0468e-01, -1.8604e-01, -7.7262e-02,\n",
      "        -4.7082e-02, -3.3199e-01, -2.6907e-03,  1.0010e-01,  7.8234e-02,\n",
      "        -8.9790e-02, -2.8957e-01,  5.9772e-02, -3.6687e-01, -1.9616e-03,\n",
      "        -2.1082e-01, -7.0088e-02, -2.6545e-01,  8.7921e-02, -1.6007e-01,\n",
      "        -4.2448e-02,  7.8112e-02,  3.7638e-01, -2.3472e-01, -4.9971e-02,\n",
      "        -2.4137e-01,  6.5733e-02, -8.6810e-02, -1.6566e-01, -9.2850e-02,\n",
      "        -1.9562e-01, -1.5094e-01, -5.7365e-02,  1.3224e-01, -5.5122e-01,\n",
      "        -2.2211e-02, -8.8627e-02, -8.4832e-02, -8.7210e-02, -9.2882e-02,\n",
      "        -4.8866e-02,  1.4268e-03, -1.9317e-01, -3.0268e-01,  2.8378e-02,\n",
      "        -3.7397e-01,  9.4343e-02, -1.0817e-01, -2.6380e-01, -2.2096e-01,\n",
      "        -3.0265e-01, -3.0556e-01, -1.8835e-01, -5.8264e-01, -9.4726e-02,\n",
      "         5.4391e-02, -2.5494e-01, -3.6798e-01, -3.2093e-01, -2.8276e-01,\n",
      "        -2.2803e-01, -2.5556e-01,  4.0431e-03, -1.7836e-01, -1.1373e-01,\n",
      "        -1.6105e-01, -2.2606e-04,  1.7048e-01, -9.4321e-03, -2.1131e-01,\n",
      "         9.1955e-01,  8.7462e-01,  8.3969e-01,  6.5406e-01,  8.0815e-01,\n",
      "         8.0829e-01,  1.0158e+00,  7.8346e-01,  8.0755e-01,  8.4958e-01,\n",
      "         1.0410e+00,  5.4430e-01,  8.1938e-01,  8.1393e-01,  6.7920e-01,\n",
      "         8.9127e-01,  8.0804e-01,  8.7469e-01,  9.3887e-01,  9.6369e-01,\n",
      "         1.1810e+00,  9.9204e-01,  8.6734e-01,  6.0897e-01,  9.0408e-01,\n",
      "         8.5600e-01,  8.1072e-01,  7.5008e-01,  8.5193e-01,  5.2214e-01,\n",
      "         8.7683e-01,  1.0565e+00,  9.8366e-01,  1.0229e+00,  9.3139e-01,\n",
      "         7.6926e-01,  3.4102e-01,  7.5753e-01,  7.0910e-01,  7.6555e-01,\n",
      "         7.4417e-01,  8.4481e-01,  9.3494e-01,  8.7219e-01,  7.5489e-01,\n",
      "         5.5206e-01,  8.3939e-01,  1.1207e+00,  5.2566e-01,  9.6002e-01,\n",
      "         9.3136e-01,  8.3750e-01,  8.5060e-01,  6.8500e-01,  8.3125e-01,\n",
      "         8.9896e-01,  6.7541e-01,  8.4103e-01,  9.4982e-01,  8.8277e-01,\n",
      "         1.1154e+00,  7.6015e-01,  6.9901e-01,  8.4076e-01,  9.8255e-01,\n",
      "         7.0692e-01,  8.1235e-01,  8.9868e-01,  5.5886e-01,  9.8942e-01,\n",
      "         7.7100e-01,  7.6458e-01,  7.2202e-01,  9.6485e-01,  8.8944e-01,\n",
      "         6.5253e-01,  9.5350e-01,  9.8521e-01,  9.4923e-01,  8.6144e-01,\n",
      "         1.0444e+00,  6.7265e-01,  7.8438e-01,  3.2153e-01,  6.1909e-01,\n",
      "         8.9529e-01,  8.6087e-01,  1.0208e+00,  9.4112e-01,  7.8444e-01,\n",
      "         1.0528e+00,  7.7176e-01,  9.6572e-01,  9.1595e-01,  8.5990e-01,\n",
      "         6.8522e-01,  7.8266e-01,  7.2438e-01,  1.0206e+00,  6.7312e-01,\n",
      "         8.3408e-01,  9.4710e-01,  9.5669e-01,  9.8791e-01,  9.1094e-01,\n",
      "         3.9980e-01,  9.2343e-01,  8.7671e-01,  7.7845e-01,  1.0315e+00,\n",
      "         9.3967e-01,  7.8909e-01,  5.7278e-01,  9.4527e-01,  7.0400e-01,\n",
      "         5.5635e-01,  1.0201e+00,  5.1228e-01,  7.1342e-01,  6.5353e-01,\n",
      "         8.8543e-01,  6.1170e-01,  8.9345e-01,  4.0607e-01,  6.4561e-01,\n",
      "         4.3239e-01,  7.7684e-01,  5.8749e-01,  7.4182e-01,  9.8855e-01,\n",
      "         8.7232e-01,  9.2578e-01,  6.5836e-01,  9.2243e-01,  7.6795e-01,\n",
      "         6.9793e-01,  1.0403e+00,  7.4481e-01,  4.6516e-01,  7.3994e-01,\n",
      "         7.8569e-01,  7.3919e-01,  7.9429e-01,  5.6388e-01,  8.4758e-01,\n",
      "         9.6997e-01,  9.1017e-01,  9.0215e-01,  4.9265e-01,  7.7935e-01,\n",
      "         7.2732e-02,  8.1613e-01,  1.0784e+00,  7.4565e-01,  8.2109e-01,\n",
      "         8.3439e-01,  9.3212e-01,  8.2079e-01,  1.1432e+00,  9.8848e-01,\n",
      "         6.1868e-01,  6.7718e-01,  8.3478e-01,  1.0196e+00,  8.9025e-01,\n",
      "         9.9054e-01,  6.1598e-01,  1.0058e+00,  9.0388e-01,  8.7458e-01,\n",
      "         1.2986e+00,  8.0378e-01,  9.7514e-01,  9.7813e-01,  8.2485e-01,\n",
      "         7.4854e-01,  6.1338e-01,  5.9496e-01,  7.8757e-01,  8.5395e-01,\n",
      "         8.6280e-01,  7.5089e-01,  9.0887e-01,  1.1476e+00,  1.0873e+00,\n",
      "         7.2777e-01,  7.4729e-01,  5.7484e-01,  1.0145e+00,  8.9294e-01,\n",
      "         7.7855e-01,  8.6505e-01,  7.2196e-01,  9.5526e-01,  8.7862e-01,\n",
      "         1.8137e-01,  7.4810e-01,  7.4602e-01,  3.0640e-01,  6.8740e-01,\n",
      "         1.4278e-03,  9.8463e-02,  5.4639e-01,  6.1884e-02, -2.7154e-01,\n",
      "         2.4213e-01,  2.3587e-01, -4.5867e-02, -2.8904e-01,  1.5456e-01,\n",
      "         3.6020e-01, -2.4376e-01, -1.0357e-01, -7.0327e-01, -1.7468e-01,\n",
      "        -7.5030e-02,  1.9776e-01, -6.9596e-01,  4.2823e-01, -2.7109e-01,\n",
      "         1.8006e-01,  8.0833e-02,  1.6613e-01,  4.3513e-01,  1.2612e-01,\n",
      "        -1.2823e-02, -5.1390e-01,  1.6000e-01, -1.9023e-01, -4.5535e-02,\n",
      "         2.7352e-01,  3.0720e-01, -3.0515e-01, -3.9164e-01, -2.0079e-01,\n",
      "        -5.7885e-01, -2.8800e-01, -1.8181e-01, -4.7485e-01, -5.6045e-01,\n",
      "         3.7940e-01,  3.8337e-01, -3.0136e-01, -6.9791e-02,  1.7122e-01,\n",
      "        -3.2325e-02,  6.8370e-02,  2.6556e-01,  1.2171e-01, -5.6097e-01,\n",
      "         4.7462e-01, -2.0043e-01, -1.9352e-01,  3.8693e-01,  2.0798e-01,\n",
      "        -1.3111e-01,  2.4645e-01,  4.8275e-01, -3.1942e-01, -3.1366e-01,\n",
      "         3.1959e-01, -8.2945e-01,  6.0894e-02, -2.6709e-01, -7.0336e-02,\n",
      "         8.1490e-02, -6.5751e-02,  3.5728e-01, -5.6530e-03,  4.1118e-01,\n",
      "        -2.3882e-01, -1.8672e-01, -2.1833e-01, -5.1440e-02, -8.1007e-03,\n",
      "         4.8670e-01, -2.8268e-02,  3.0911e-01,  1.5644e-01,  1.0762e-01,\n",
      "         3.9918e-01,  9.9574e-02,  2.7025e-02,  7.3850e-01, -8.5950e-01,\n",
      "        -6.1311e-01, -9.8304e-02,  2.1872e-01, -8.4889e-02,  3.8473e-01,\n",
      "         1.9260e-01,  6.5761e-01,  4.0395e-01, -7.1151e-01, -2.8296e-01,\n",
      "        -2.8526e-01, -2.7861e-01, -3.7150e-01,  3.1986e-01,  6.0759e-01,\n",
      "         4.5411e-01,  1.3383e-01,  1.9350e-01,  3.6184e-02,  3.3120e-01,\n",
      "         3.5278e-01, -3.9608e-01, -5.2399e-01, -1.0331e-01, -2.6657e-01,\n",
      "         2.1334e-01, -1.8702e-01,  4.9889e-02,  1.6232e-01, -3.5800e-01,\n",
      "         3.4583e-02,  4.3510e-02, -2.4906e-01, -2.7981e-01,  1.3931e-01,\n",
      "        -5.5007e-01, -4.6789e-01, -1.9660e-01, -2.6802e-01,  3.9671e-02,\n",
      "         2.8194e-01,  6.5404e-01, -1.4117e-01, -3.1999e-01, -1.2291e-02,\n",
      "         2.2953e-01,  1.8556e-01,  5.0388e-01,  6.1848e-01, -4.9514e-01,\n",
      "        -3.6202e-01, -1.0448e-01,  4.5127e-01, -3.0947e-01, -1.7481e-01,\n",
      "         1.6442e-02,  4.1746e-01, -6.6682e-01,  5.0309e-01, -2.6276e-01,\n",
      "        -5.2682e-01, -1.2662e-01, -7.1933e-01,  2.6989e-01,  5.7127e-01,\n",
      "        -4.9057e-01,  5.7128e-03,  6.5044e-01,  1.5832e-01, -3.9113e-01,\n",
      "         9.3375e-02, -5.4186e-01, -4.3874e-02, -7.4900e-02, -4.2908e-02,\n",
      "         7.0391e-02,  1.0723e-01, -5.4150e-02, -2.3840e-01, -6.0680e-02,\n",
      "        -7.2016e-02, -1.1598e-01, -3.0908e-01, -1.6452e-01, -1.1974e-01,\n",
      "         1.8345e-01,  9.4208e-02, -2.9529e-01,  2.0711e-02, -5.7070e-01,\n",
      "         2.4096e-01,  8.5671e-02,  4.8009e-01, -1.2553e-01,  3.0733e-01,\n",
      "         3.7943e-01, -2.8814e-02, -2.0238e-01, -1.0263e-01, -4.4182e-01,\n",
      "         7.8051e-01,  6.2551e-01,  2.7812e-02, -1.6324e-01,  2.3333e-02,\n",
      "         1.9665e-01, -1.3114e-01, -1.6127e-02, -7.5951e-02, -7.1420e-01,\n",
      "        -2.0077e-02,  1.9682e-01,  3.1428e-01, -1.1866e-02,  3.3046e-01,\n",
      "        -6.0140e-01, -3.0346e-01, -4.1196e-01, -2.5876e-01, -1.2890e-01,\n",
      "        -5.0165e-01, -3.4487e-01, -2.5679e-01, -2.5510e-01, -3.1141e-01,\n",
      "        -9.6256e-01, -1.6112e-01,  2.7624e-02, -1.0790e-01, -3.1275e-01,\n",
      "        -4.4154e-01, -1.7567e-01, -1.5342e-01, -3.9169e-01, -6.7650e-01,\n",
      "        -5.0304e-01, -1.9964e-02, -2.5868e-01, -3.1179e-01, -2.0838e-01,\n",
      "        -5.8770e-01, -3.9910e-01, -3.5618e-01, -4.9050e-01, -2.9422e-01,\n",
      "        -2.4247e-01, -2.8429e-01, -3.6677e-02, -2.2885e-01, -5.6926e-01,\n",
      "        -2.9966e-01, -1.2426e-01, -3.0807e-01, -2.8464e-01, -4.3549e-01,\n",
      "        -6.9063e-01, -3.4259e-01, -5.6359e-01, -2.4922e-01, -5.0297e-01,\n",
      "        -4.3372e-01, -3.6401e-01, -1.2139e-01, -1.3983e-01, -2.0886e-01,\n",
      "        -4.1179e-01, -4.8034e-01, -3.4580e-01, -5.1785e-01, -1.5049e-01,\n",
      "        -1.6592e-01, -3.1400e-01, -9.5982e-01, -1.2847e-01, -2.0299e-01,\n",
      "        -7.6757e-01, -2.6631e-01, -2.3318e-01, -1.2092e-01, -1.5641e-01,\n",
      "        -6.7965e-01, -3.4488e-01, -5.3538e-01, -7.4866e-02, -2.7088e-01,\n",
      "        -3.8774e-01, -4.4732e-01, -2.4539e-01, -6.5708e-01, -3.7782e-01,\n",
      "        -3.6176e-01, -3.5715e-02, -8.6624e-02, -1.0403e-01, -3.0789e-01,\n",
      "        -7.6411e-01, -4.6061e-01, -3.6659e-01, -4.1470e-01, -5.4956e-01,\n",
      "        -6.6727e-02, -3.1873e-01, -2.6416e-01, -3.8453e-01, -1.5477e-01,\n",
      "        -5.5524e-01, -2.5042e-01, -5.6192e-01, -5.6467e-01, -3.2564e-01,\n",
      "        -1.9211e-01, -4.3016e-01, -5.0237e-01, -3.6818e-01, -4.0939e-01,\n",
      "        -3.5111e-01, -2.9284e-01, -4.0926e-01, -1.7565e-01, -6.1417e-01,\n",
      "        -2.8147e-02, -2.2463e-01, -1.4299e-01, -3.6546e-01, -5.5246e-01,\n",
      "        -2.2649e-01, -5.4433e-01, -6.5060e-01, -6.8854e-01, -1.6636e-01,\n",
      "        -1.0364e-01, -7.8424e-01, -4.5118e-01, -4.2304e-01, -6.6483e-01,\n",
      "        -2.6467e-01, -6.1959e-02, -2.6587e-01, -2.1531e-01, -4.8208e-01,\n",
      "        -5.9461e-01, -7.5356e-01, -3.1838e-01, -4.3683e-01, -3.5605e-01,\n",
      "        -4.2825e-01, -1.6073e-01, -3.9383e-01, -2.3532e-01, -4.8675e-01,\n",
      "        -4.0493e-01, -3.4288e-01, -3.1979e-01, -1.9949e-01, -4.0583e-01,\n",
      "        -4.6598e-01, -3.7904e-01, -8.3139e-01, -7.3022e-01, -2.8643e-01,\n",
      "        -3.2406e-01, -2.7547e-01, -5.1462e-01, -3.4213e-01, -3.7178e-01,\n",
      "        -4.9805e-01, -1.0484e-01, -1.0608e+00, -3.8095e-01, -3.9614e-01,\n",
      "        -4.4622e-01, -3.3181e-01, -5.6958e-01, -5.4431e-01, -1.5188e-01,\n",
      "        -3.1286e-01, -4.0539e-01, -7.5996e-01, -2.4477e-01, -7.9193e-01,\n",
      "        -4.4644e-02, -2.5442e-01, -4.9655e-01, -3.0449e-01, -2.5090e-01,\n",
      "        -8.9006e-02, -3.3069e-01, -5.9929e-01, -3.3034e-01, -4.4092e-01,\n",
      "        -4.0550e-01, -1.8025e-01, -3.6270e-01, -3.3770e-01, -4.7527e-01,\n",
      "        -3.6574e-01, -5.4066e-01, -3.4407e-01, -8.4674e-01, -7.0090e-01,\n",
      "        -5.8012e-01, -4.0686e-01, -4.0257e-01, -4.4235e-01, -2.7991e-01,\n",
      "        -7.5184e-01, -5.8519e-01, -2.3481e-01, -4.4350e-01, -2.2110e-01,\n",
      "        -1.7259e-01, -2.2409e-01,  1.8470e-01, -1.8290e-01, -6.1163e-01],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1712,  0.0107, -0.2691,  ..., -0.1808,  0.0236,  0.0767],\n",
      "        [ 0.1208, -0.0461, -0.3767,  ...,  0.1101, -0.0866,  0.1438],\n",
      "        [-0.0330, -0.1798, -0.1822,  ...,  0.1745,  0.0842,  0.2194],\n",
      "        ...,\n",
      "        [-0.2405,  0.0352, -0.0174,  ..., -0.1192,  0.0259, -0.0172],\n",
      "        [-0.1533, -0.2387, -0.2333,  ..., -0.4458, -0.1950, -0.5138],\n",
      "        [-0.5754,  0.0812, -0.3047,  ..., -0.1104, -0.0972, -0.0433]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1195, -0.0338, -0.0382,  ...,  0.0069, -0.0903,  0.0172],\n",
      "        [ 0.0129,  0.1198,  0.0455,  ..., -0.0194, -0.2333, -0.0495],\n",
      "        [-0.0665,  0.0338,  0.0335,  ...,  0.0436,  0.0250,  0.0060],\n",
      "        ...,\n",
      "        [-0.1007, -0.1549, -0.0116,  ...,  0.1420, -0.0751, -0.0625],\n",
      "        [ 0.0638,  0.0208, -0.1794,  ...,  0.0308, -0.2056,  0.0404],\n",
      "        [-0.0954, -0.0347, -0.2762,  ...,  0.0495,  0.0600, -0.0941]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-9.7684e-02, -1.9035e-01, -1.7424e-01, -9.7548e-02,  6.6590e-02,\n",
      "        -5.5609e-02, -4.6714e-01,  2.0046e-02, -3.4165e-01,  2.1935e-01,\n",
      "        -3.1880e-01,  5.8849e-02,  3.6246e-02,  5.1916e-02, -1.5045e-01,\n",
      "        -2.9095e-01, -6.2016e-02, -1.9516e-01, -2.6502e-02, -3.0087e-01,\n",
      "         2.5273e-01, -3.3973e-02, -5.4771e-02, -6.9037e-02,  1.3471e-01,\n",
      "        -3.0035e-01, -1.0811e-01, -9.7404e-02, -2.0780e-02, -3.4107e-01,\n",
      "        -2.7319e-01, -2.9391e-01, -5.6123e-02, -2.4450e-01, -2.4024e-01,\n",
      "        -1.7200e-01,  1.2291e-04, -1.8925e-01, -2.0725e-01, -1.2898e-01,\n",
      "        -2.0673e-01, -3.7136e-01, -4.6411e-01, -1.6745e-01, -1.0909e-01,\n",
      "        -3.7188e-01, -4.9178e-01, -1.4460e-01,  1.2909e-01, -2.6426e-01,\n",
      "        -6.3116e-01, -5.6399e-01,  3.3960e-03, -5.7922e-01, -1.9177e-01,\n",
      "         1.3243e-01,  1.9788e-02, -7.8850e-02, -1.9224e-01,  3.4999e-02,\n",
      "        -2.6429e-01,  5.1717e-02, -2.7240e-01, -1.8205e-01, -1.6320e-01,\n",
      "         8.3001e-02,  4.9224e-02, -1.3526e-01, -6.0816e-02, -2.3948e-01,\n",
      "         1.6153e-02, -6.6850e-02, -1.1093e-02, -7.8766e-02,  1.7077e-01,\n",
      "        -1.0369e-01, -1.6557e-01, -1.8078e-01,  1.1637e-01, -3.7464e-02,\n",
      "        -3.1939e-02,  4.3010e-02, -2.8061e-01, -1.2236e-01, -3.3591e-01,\n",
      "         5.3721e-03, -4.0011e-02,  1.5059e-02, -3.6432e-01, -1.4011e-01,\n",
      "        -1.6347e-01, -1.2084e-01,  4.4030e-02,  1.0505e-01, -6.2153e-02,\n",
      "         3.4240e-02, -1.7811e-01,  1.1578e-02, -1.9444e-01,  8.8057e-02,\n",
      "        -2.0736e-01, -1.0122e-01, -8.7484e-02,  2.6307e-03, -2.8240e-01,\n",
      "        -5.6065e-02,  1.6202e-01, -2.8625e-01, -1.3773e-01, -4.5850e-01,\n",
      "        -1.8073e-01, -1.9471e-01, -9.6289e-02, -2.3539e-01, -8.8841e-02,\n",
      "        -4.9664e-03, -2.4626e-01, -3.2135e-01, -3.1180e-01, -4.0575e-01,\n",
      "         9.9701e-02,  6.3277e-02, -1.7694e-04,  1.6959e-01, -7.1661e-01,\n",
      "        -4.2106e-01,  4.1162e-02, -8.7729e-02, -4.9849e-01, -2.5781e-01,\n",
      "         5.7233e-02,  2.3700e-01, -2.0468e-01, -1.8604e-01, -7.7262e-02,\n",
      "        -4.7082e-02, -3.3199e-01, -2.6907e-03,  1.0010e-01,  7.8234e-02,\n",
      "        -8.9790e-02, -2.8957e-01,  5.9772e-02, -3.6687e-01, -1.9616e-03,\n",
      "        -2.1082e-01, -7.0088e-02, -2.6545e-01,  8.7921e-02, -1.6007e-01,\n",
      "        -4.2448e-02,  7.8112e-02,  3.7638e-01, -2.3472e-01, -4.9971e-02,\n",
      "        -2.4137e-01,  6.5733e-02, -8.6810e-02, -1.6566e-01, -9.2850e-02,\n",
      "        -1.9562e-01, -1.5094e-01, -5.7365e-02,  1.3224e-01, -5.5122e-01,\n",
      "        -2.2211e-02, -8.8627e-02, -8.4832e-02, -8.7210e-02, -9.2882e-02,\n",
      "        -4.8866e-02,  1.4268e-03, -1.9317e-01, -3.0268e-01,  2.8378e-02,\n",
      "        -3.7397e-01,  9.4343e-02, -1.0817e-01, -2.6380e-01, -2.2096e-01,\n",
      "        -3.0265e-01, -3.0556e-01, -1.8835e-01, -5.8264e-01, -9.4726e-02,\n",
      "         5.4391e-02, -2.5494e-01, -3.6798e-01, -3.2093e-01, -2.8276e-01,\n",
      "        -2.2803e-01, -2.5556e-01,  4.0430e-03, -1.7836e-01, -1.1373e-01,\n",
      "        -1.6105e-01, -2.2607e-04,  1.7048e-01, -9.4321e-03, -2.1131e-01,\n",
      "         9.1955e-01,  8.7462e-01,  8.3969e-01,  6.5406e-01,  8.0815e-01,\n",
      "         8.0829e-01,  1.0158e+00,  7.8346e-01,  8.0755e-01,  8.4958e-01,\n",
      "         1.0410e+00,  5.4430e-01,  8.1938e-01,  8.1393e-01,  6.7920e-01,\n",
      "         8.9127e-01,  8.0804e-01,  8.7469e-01,  9.3887e-01,  9.6369e-01,\n",
      "         1.1810e+00,  9.9204e-01,  8.6734e-01,  6.0897e-01,  9.0408e-01,\n",
      "         8.5600e-01,  8.1072e-01,  7.5008e-01,  8.5193e-01,  5.2214e-01,\n",
      "         8.7683e-01,  1.0565e+00,  9.8366e-01,  1.0229e+00,  9.3139e-01,\n",
      "         7.6926e-01,  3.4102e-01,  7.5753e-01,  7.0910e-01,  7.6555e-01,\n",
      "         7.4417e-01,  8.4481e-01,  9.3494e-01,  8.7219e-01,  7.5489e-01,\n",
      "         5.5206e-01,  8.3939e-01,  1.1207e+00,  5.2566e-01,  9.6002e-01,\n",
      "         9.3136e-01,  8.3750e-01,  8.5060e-01,  6.8500e-01,  8.3125e-01,\n",
      "         8.9896e-01,  6.7541e-01,  8.4103e-01,  9.4982e-01,  8.8277e-01,\n",
      "         1.1154e+00,  7.6015e-01,  6.9901e-01,  8.4076e-01,  9.8255e-01,\n",
      "         7.0692e-01,  8.1235e-01,  8.9868e-01,  5.5886e-01,  9.8942e-01,\n",
      "         7.7100e-01,  7.6458e-01,  7.2202e-01,  9.6485e-01,  8.8944e-01,\n",
      "         6.5253e-01,  9.5350e-01,  9.8521e-01,  9.4923e-01,  8.6144e-01,\n",
      "         1.0444e+00,  6.7265e-01,  7.8438e-01,  3.2153e-01,  6.1909e-01,\n",
      "         8.9529e-01,  8.6087e-01,  1.0208e+00,  9.4112e-01,  7.8444e-01,\n",
      "         1.0528e+00,  7.7176e-01,  9.6572e-01,  9.1595e-01,  8.5990e-01,\n",
      "         6.8522e-01,  7.8266e-01,  7.2438e-01,  1.0206e+00,  6.7312e-01,\n",
      "         8.3408e-01,  9.4710e-01,  9.5669e-01,  9.8791e-01,  9.1094e-01,\n",
      "         3.9980e-01,  9.2343e-01,  8.7671e-01,  7.7845e-01,  1.0315e+00,\n",
      "         9.3967e-01,  7.8909e-01,  5.7278e-01,  9.4527e-01,  7.0400e-01,\n",
      "         5.5635e-01,  1.0201e+00,  5.1228e-01,  7.1342e-01,  6.5353e-01,\n",
      "         8.8543e-01,  6.1170e-01,  8.9345e-01,  4.0607e-01,  6.4561e-01,\n",
      "         4.3239e-01,  7.7684e-01,  5.8749e-01,  7.4182e-01,  9.8855e-01,\n",
      "         8.7232e-01,  9.2578e-01,  6.5836e-01,  9.2243e-01,  7.6795e-01,\n",
      "         6.9793e-01,  1.0403e+00,  7.4481e-01,  4.6516e-01,  7.3994e-01,\n",
      "         7.8569e-01,  7.3919e-01,  7.9429e-01,  5.6388e-01,  8.4758e-01,\n",
      "         9.6997e-01,  9.1017e-01,  9.0215e-01,  4.9265e-01,  7.7935e-01,\n",
      "         7.2732e-02,  8.1613e-01,  1.0784e+00,  7.4565e-01,  8.2109e-01,\n",
      "         8.3439e-01,  9.3212e-01,  8.2079e-01,  1.1432e+00,  9.8848e-01,\n",
      "         6.1868e-01,  6.7718e-01,  8.3478e-01,  1.0196e+00,  8.9025e-01,\n",
      "         9.9054e-01,  6.1598e-01,  1.0058e+00,  9.0388e-01,  8.7458e-01,\n",
      "         1.2986e+00,  8.0378e-01,  9.7514e-01,  9.7813e-01,  8.2485e-01,\n",
      "         7.4854e-01,  6.1338e-01,  5.9496e-01,  7.8757e-01,  8.5395e-01,\n",
      "         8.6280e-01,  7.5089e-01,  9.0887e-01,  1.1476e+00,  1.0873e+00,\n",
      "         7.2777e-01,  7.4729e-01,  5.7484e-01,  1.0145e+00,  8.9294e-01,\n",
      "         7.7855e-01,  8.6505e-01,  7.2196e-01,  9.5526e-01,  8.7862e-01,\n",
      "         1.8137e-01,  7.4810e-01,  7.4602e-01,  3.0640e-01,  6.8740e-01,\n",
      "         1.4279e-03,  9.8463e-02,  5.4639e-01,  6.1884e-02, -2.7154e-01,\n",
      "         2.4213e-01,  2.3587e-01, -4.5867e-02, -2.8904e-01,  1.5456e-01,\n",
      "         3.6020e-01, -2.4376e-01, -1.0357e-01, -7.0327e-01, -1.7468e-01,\n",
      "        -7.5030e-02,  1.9776e-01, -6.9596e-01,  4.2823e-01, -2.7109e-01,\n",
      "         1.8006e-01,  8.0833e-02,  1.6613e-01,  4.3513e-01,  1.2612e-01,\n",
      "        -1.2823e-02, -5.1390e-01,  1.6000e-01, -1.9023e-01, -4.5535e-02,\n",
      "         2.7352e-01,  3.0720e-01, -3.0515e-01, -3.9164e-01, -2.0079e-01,\n",
      "        -5.7885e-01, -2.8800e-01, -1.8181e-01, -4.7485e-01, -5.6045e-01,\n",
      "         3.7940e-01,  3.8337e-01, -3.0136e-01, -6.9791e-02,  1.7122e-01,\n",
      "        -3.2325e-02,  6.8370e-02,  2.6556e-01,  1.2171e-01, -5.6097e-01,\n",
      "         4.7462e-01, -2.0043e-01, -1.9352e-01,  3.8693e-01,  2.0798e-01,\n",
      "        -1.3111e-01,  2.4645e-01,  4.8275e-01, -3.1942e-01, -3.1366e-01,\n",
      "         3.1959e-01, -8.2945e-01,  6.0894e-02, -2.6709e-01, -7.0336e-02,\n",
      "         8.1490e-02, -6.5751e-02,  3.5728e-01, -5.6530e-03,  4.1118e-01,\n",
      "        -2.3882e-01, -1.8672e-01, -2.1833e-01, -5.1440e-02, -8.1007e-03,\n",
      "         4.8670e-01, -2.8268e-02,  3.0911e-01,  1.5644e-01,  1.0762e-01,\n",
      "         3.9918e-01,  9.9574e-02,  2.7025e-02,  7.3850e-01, -8.5950e-01,\n",
      "        -6.1311e-01, -9.8304e-02,  2.1872e-01, -8.4889e-02,  3.8473e-01,\n",
      "         1.9260e-01,  6.5761e-01,  4.0395e-01, -7.1151e-01, -2.8296e-01,\n",
      "        -2.8526e-01, -2.7861e-01, -3.7150e-01,  3.1986e-01,  6.0759e-01,\n",
      "         4.5411e-01,  1.3383e-01,  1.9350e-01,  3.6184e-02,  3.3120e-01,\n",
      "         3.5278e-01, -3.9608e-01, -5.2399e-01, -1.0331e-01, -2.6657e-01,\n",
      "         2.1334e-01, -1.8702e-01,  4.9889e-02,  1.6232e-01, -3.5800e-01,\n",
      "         3.4583e-02,  4.3510e-02, -2.4906e-01, -2.7981e-01,  1.3931e-01,\n",
      "        -5.5007e-01, -4.6789e-01, -1.9660e-01, -2.6802e-01,  3.9671e-02,\n",
      "         2.8194e-01,  6.5404e-01, -1.4117e-01, -3.1999e-01, -1.2291e-02,\n",
      "         2.2953e-01,  1.8556e-01,  5.0388e-01,  6.1848e-01, -4.9514e-01,\n",
      "        -3.6202e-01, -1.0448e-01,  4.5127e-01, -3.0947e-01, -1.7481e-01,\n",
      "         1.6442e-02,  4.1746e-01, -6.6682e-01,  5.0309e-01, -2.6276e-01,\n",
      "        -5.2682e-01, -1.2662e-01, -7.1933e-01,  2.6989e-01,  5.7127e-01,\n",
      "        -4.9057e-01,  5.7127e-03,  6.5044e-01,  1.5832e-01, -3.9113e-01,\n",
      "         9.3375e-02, -5.4186e-01, -4.3874e-02, -7.4900e-02, -4.2908e-02,\n",
      "         7.0391e-02,  1.0723e-01, -5.4150e-02, -2.3840e-01, -6.0680e-02,\n",
      "        -7.2016e-02, -1.1598e-01, -3.0908e-01, -1.6452e-01, -1.1974e-01,\n",
      "         1.8345e-01,  9.4208e-02, -2.9529e-01,  2.0711e-02, -5.7070e-01,\n",
      "         2.4096e-01,  8.5671e-02,  4.8009e-01, -1.2553e-01,  3.0733e-01,\n",
      "         3.7943e-01, -2.8814e-02, -2.0238e-01, -1.0263e-01, -4.4182e-01,\n",
      "         7.8051e-01,  6.2551e-01,  2.7812e-02, -1.6324e-01,  2.3333e-02,\n",
      "         1.9665e-01, -1.3114e-01, -1.6127e-02, -7.5951e-02, -7.1420e-01,\n",
      "        -2.0077e-02,  1.9682e-01,  3.1428e-01, -1.1866e-02,  3.3046e-01,\n",
      "        -6.0140e-01, -3.0346e-01, -4.1196e-01, -2.5876e-01, -1.2890e-01,\n",
      "        -5.0165e-01, -3.4487e-01, -2.5679e-01, -2.5510e-01, -3.1141e-01,\n",
      "        -9.6256e-01, -1.6112e-01,  2.7624e-02, -1.0790e-01, -3.1275e-01,\n",
      "        -4.4154e-01, -1.7567e-01, -1.5342e-01, -3.9169e-01, -6.7650e-01,\n",
      "        -5.0304e-01, -1.9964e-02, -2.5868e-01, -3.1179e-01, -2.0838e-01,\n",
      "        -5.8770e-01, -3.9910e-01, -3.5618e-01, -4.9050e-01, -2.9422e-01,\n",
      "        -2.4247e-01, -2.8429e-01, -3.6677e-02, -2.2885e-01, -5.6926e-01,\n",
      "        -2.9966e-01, -1.2426e-01, -3.0807e-01, -2.8464e-01, -4.3549e-01,\n",
      "        -6.9063e-01, -3.4259e-01, -5.6359e-01, -2.4922e-01, -5.0297e-01,\n",
      "        -4.3372e-01, -3.6401e-01, -1.2139e-01, -1.3983e-01, -2.0886e-01,\n",
      "        -4.1179e-01, -4.8034e-01, -3.4580e-01, -5.1785e-01, -1.5049e-01,\n",
      "        -1.6592e-01, -3.1400e-01, -9.5982e-01, -1.2847e-01, -2.0299e-01,\n",
      "        -7.6757e-01, -2.6631e-01, -2.3318e-01, -1.2092e-01, -1.5641e-01,\n",
      "        -6.7965e-01, -3.4488e-01, -5.3538e-01, -7.4866e-02, -2.7088e-01,\n",
      "        -3.8774e-01, -4.4732e-01, -2.4539e-01, -6.5708e-01, -3.7782e-01,\n",
      "        -3.6176e-01, -3.5715e-02, -8.6624e-02, -1.0403e-01, -3.0789e-01,\n",
      "        -7.6411e-01, -4.6061e-01, -3.6659e-01, -4.1470e-01, -5.4956e-01,\n",
      "        -6.6727e-02, -3.1873e-01, -2.6416e-01, -3.8453e-01, -1.5477e-01,\n",
      "        -5.5524e-01, -2.5042e-01, -5.6192e-01, -5.6467e-01, -3.2564e-01,\n",
      "        -1.9211e-01, -4.3016e-01, -5.0237e-01, -3.6818e-01, -4.0939e-01,\n",
      "        -3.5111e-01, -2.9284e-01, -4.0926e-01, -1.7565e-01, -6.1417e-01,\n",
      "        -2.8147e-02, -2.2463e-01, -1.4299e-01, -3.6546e-01, -5.5246e-01,\n",
      "        -2.2649e-01, -5.4433e-01, -6.5060e-01, -6.8854e-01, -1.6636e-01,\n",
      "        -1.0364e-01, -7.8424e-01, -4.5118e-01, -4.2304e-01, -6.6483e-01,\n",
      "        -2.6467e-01, -6.1959e-02, -2.6587e-01, -2.1531e-01, -4.8208e-01,\n",
      "        -5.9461e-01, -7.5356e-01, -3.1838e-01, -4.3683e-01, -3.5605e-01,\n",
      "        -4.2825e-01, -1.6073e-01, -3.9383e-01, -2.3532e-01, -4.8675e-01,\n",
      "        -4.0493e-01, -3.4288e-01, -3.1979e-01, -1.9949e-01, -4.0583e-01,\n",
      "        -4.6598e-01, -3.7904e-01, -8.3139e-01, -7.3022e-01, -2.8643e-01,\n",
      "        -3.2406e-01, -2.7547e-01, -5.1462e-01, -3.4213e-01, -3.7178e-01,\n",
      "        -4.9805e-01, -1.0484e-01, -1.0608e+00, -3.8095e-01, -3.9614e-01,\n",
      "        -4.4622e-01, -3.3181e-01, -5.6958e-01, -5.4431e-01, -1.5188e-01,\n",
      "        -3.1286e-01, -4.0539e-01, -7.5996e-01, -2.4477e-01, -7.9193e-01,\n",
      "        -4.4644e-02, -2.5442e-01, -4.9655e-01, -3.0449e-01, -2.5090e-01,\n",
      "        -8.9006e-02, -3.3069e-01, -5.9929e-01, -3.3034e-01, -4.4092e-01,\n",
      "        -4.0550e-01, -1.8025e-01, -3.6270e-01, -3.3770e-01, -4.7527e-01,\n",
      "        -3.6574e-01, -5.4066e-01, -3.4407e-01, -8.4674e-01, -7.0090e-01,\n",
      "        -5.8012e-01, -4.0686e-01, -4.0257e-01, -4.4235e-01, -2.7991e-01,\n",
      "        -7.5184e-01, -5.8519e-01, -2.3481e-01, -4.4350e-01, -2.2110e-01,\n",
      "        -1.7259e-01, -2.2409e-01,  1.8470e-01, -1.8290e-01, -6.1163e-01],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-9.7684e-02, -1.9035e-01, -1.7424e-01, -9.7548e-02,  6.6590e-02,\n",
      "        -5.5609e-02, -4.6714e-01,  2.0046e-02, -3.4165e-01,  2.1935e-01,\n",
      "        -3.1880e-01,  5.8849e-02,  3.6245e-02,  5.1916e-02, -1.5045e-01,\n",
      "        -2.9095e-01, -6.2016e-02, -1.9516e-01, -2.6502e-02, -3.0087e-01,\n",
      "         2.5273e-01, -3.3973e-02, -5.4771e-02, -6.9037e-02,  1.3471e-01,\n",
      "        -3.0035e-01, -1.0811e-01, -9.7404e-02, -2.0780e-02, -3.4107e-01,\n",
      "        -2.7319e-01, -2.9391e-01, -5.6123e-02, -2.4450e-01, -2.4024e-01,\n",
      "        -1.7200e-01,  1.2293e-04, -1.8925e-01, -2.0725e-01, -1.2898e-01,\n",
      "        -2.0673e-01, -3.7136e-01, -4.6411e-01, -1.6745e-01, -1.0909e-01,\n",
      "        -3.7188e-01, -4.9178e-01, -1.4460e-01,  1.2909e-01, -2.6426e-01,\n",
      "        -6.3116e-01, -5.6399e-01,  3.3961e-03, -5.7922e-01, -1.9177e-01,\n",
      "         1.3243e-01,  1.9788e-02, -7.8850e-02, -1.9224e-01,  3.4999e-02,\n",
      "        -2.6429e-01,  5.1717e-02, -2.7240e-01, -1.8205e-01, -1.6320e-01,\n",
      "         8.3001e-02,  4.9224e-02, -1.3526e-01, -6.0816e-02, -2.3948e-01,\n",
      "         1.6153e-02, -6.6850e-02, -1.1093e-02, -7.8767e-02,  1.7077e-01,\n",
      "        -1.0369e-01, -1.6557e-01, -1.8078e-01,  1.1637e-01, -3.7464e-02,\n",
      "        -3.1939e-02,  4.3010e-02, -2.8061e-01, -1.2236e-01, -3.3591e-01,\n",
      "         5.3721e-03, -4.0011e-02,  1.5059e-02, -3.6432e-01, -1.4011e-01,\n",
      "        -1.6347e-01, -1.2084e-01,  4.4030e-02,  1.0505e-01, -6.2153e-02,\n",
      "         3.4240e-02, -1.7811e-01,  1.1578e-02, -1.9444e-01,  8.8057e-02,\n",
      "        -2.0736e-01, -1.0122e-01, -8.7484e-02,  2.6308e-03, -2.8240e-01,\n",
      "        -5.6065e-02,  1.6202e-01, -2.8625e-01, -1.3773e-01, -4.5850e-01,\n",
      "        -1.8073e-01, -1.9471e-01, -9.6289e-02, -2.3539e-01, -8.8841e-02,\n",
      "        -4.9664e-03, -2.4626e-01, -3.2135e-01, -3.1180e-01, -4.0575e-01,\n",
      "         9.9702e-02,  6.3277e-02, -1.7696e-04,  1.6959e-01, -7.1660e-01,\n",
      "        -4.2106e-01,  4.1162e-02, -8.7729e-02, -4.9849e-01, -2.5781e-01,\n",
      "         5.7233e-02,  2.3700e-01, -2.0468e-01, -1.8604e-01, -7.7262e-02,\n",
      "        -4.7082e-02, -3.3199e-01, -2.6907e-03,  1.0010e-01,  7.8234e-02,\n",
      "        -8.9790e-02, -2.8957e-01,  5.9772e-02, -3.6687e-01, -1.9616e-03,\n",
      "        -2.1082e-01, -7.0088e-02, -2.6545e-01,  8.7921e-02, -1.6007e-01,\n",
      "        -4.2448e-02,  7.8112e-02,  3.7638e-01, -2.3472e-01, -4.9971e-02,\n",
      "        -2.4137e-01,  6.5733e-02, -8.6810e-02, -1.6566e-01, -9.2850e-02,\n",
      "        -1.9562e-01, -1.5094e-01, -5.7365e-02,  1.3224e-01, -5.5122e-01,\n",
      "        -2.2211e-02, -8.8627e-02, -8.4832e-02, -8.7210e-02, -9.2882e-02,\n",
      "        -4.8866e-02,  1.4268e-03, -1.9317e-01, -3.0268e-01,  2.8378e-02,\n",
      "        -3.7397e-01,  9.4343e-02, -1.0817e-01, -2.6380e-01, -2.2096e-01,\n",
      "        -3.0265e-01, -3.0556e-01, -1.8835e-01, -5.8264e-01, -9.4726e-02,\n",
      "         5.4391e-02, -2.5494e-01, -3.6798e-01, -3.2093e-01, -2.8276e-01,\n",
      "        -2.2803e-01, -2.5556e-01,  4.0431e-03, -1.7836e-01, -1.1373e-01,\n",
      "        -1.6105e-01, -2.2606e-04,  1.7048e-01, -9.4321e-03, -2.1131e-01,\n",
      "         9.1955e-01,  8.7462e-01,  8.3969e-01,  6.5406e-01,  8.0815e-01,\n",
      "         8.0829e-01,  1.0158e+00,  7.8346e-01,  8.0755e-01,  8.4958e-01,\n",
      "         1.0410e+00,  5.4430e-01,  8.1938e-01,  8.1393e-01,  6.7920e-01,\n",
      "         8.9127e-01,  8.0804e-01,  8.7469e-01,  9.3887e-01,  9.6369e-01,\n",
      "         1.1810e+00,  9.9204e-01,  8.6734e-01,  6.0897e-01,  9.0408e-01,\n",
      "         8.5600e-01,  8.1072e-01,  7.5008e-01,  8.5193e-01,  5.2214e-01,\n",
      "         8.7683e-01,  1.0565e+00,  9.8366e-01,  1.0229e+00,  9.3139e-01,\n",
      "         7.6926e-01,  3.4102e-01,  7.5753e-01,  7.0910e-01,  7.6555e-01,\n",
      "         7.4417e-01,  8.4481e-01,  9.3494e-01,  8.7219e-01,  7.5489e-01,\n",
      "         5.5206e-01,  8.3939e-01,  1.1207e+00,  5.2566e-01,  9.6002e-01,\n",
      "         9.3136e-01,  8.3750e-01,  8.5060e-01,  6.8500e-01,  8.3125e-01,\n",
      "         8.9896e-01,  6.7541e-01,  8.4103e-01,  9.4982e-01,  8.8277e-01,\n",
      "         1.1154e+00,  7.6015e-01,  6.9901e-01,  8.4076e-01,  9.8255e-01,\n",
      "         7.0692e-01,  8.1235e-01,  8.9868e-01,  5.5886e-01,  9.8942e-01,\n",
      "         7.7100e-01,  7.6458e-01,  7.2202e-01,  9.6485e-01,  8.8944e-01,\n",
      "         6.5253e-01,  9.5350e-01,  9.8521e-01,  9.4923e-01,  8.6144e-01,\n",
      "         1.0444e+00,  6.7265e-01,  7.8438e-01,  3.2153e-01,  6.1909e-01,\n",
      "         8.9529e-01,  8.6087e-01,  1.0208e+00,  9.4112e-01,  7.8444e-01,\n",
      "         1.0528e+00,  7.7176e-01,  9.6572e-01,  9.1595e-01,  8.5990e-01,\n",
      "         6.8522e-01,  7.8266e-01,  7.2438e-01,  1.0206e+00,  6.7312e-01,\n",
      "         8.3408e-01,  9.4710e-01,  9.5669e-01,  9.8791e-01,  9.1094e-01,\n",
      "         3.9980e-01,  9.2343e-01,  8.7671e-01,  7.7845e-01,  1.0315e+00,\n",
      "         9.3967e-01,  7.8909e-01,  5.7278e-01,  9.4527e-01,  7.0400e-01,\n",
      "         5.5635e-01,  1.0201e+00,  5.1228e-01,  7.1342e-01,  6.5353e-01,\n",
      "         8.8543e-01,  6.1170e-01,  8.9345e-01,  4.0607e-01,  6.4561e-01,\n",
      "         4.3239e-01,  7.7684e-01,  5.8749e-01,  7.4182e-01,  9.8855e-01,\n",
      "         8.7232e-01,  9.2578e-01,  6.5836e-01,  9.2243e-01,  7.6795e-01,\n",
      "         6.9793e-01,  1.0403e+00,  7.4481e-01,  4.6516e-01,  7.3994e-01,\n",
      "         7.8569e-01,  7.3919e-01,  7.9429e-01,  5.6388e-01,  8.4758e-01,\n",
      "         9.6997e-01,  9.1017e-01,  9.0215e-01,  4.9265e-01,  7.7935e-01,\n",
      "         7.2732e-02,  8.1613e-01,  1.0784e+00,  7.4565e-01,  8.2109e-01,\n",
      "         8.3439e-01,  9.3212e-01,  8.2079e-01,  1.1432e+00,  9.8848e-01,\n",
      "         6.1868e-01,  6.7718e-01,  8.3478e-01,  1.0196e+00,  8.9025e-01,\n",
      "         9.9054e-01,  6.1598e-01,  1.0058e+00,  9.0388e-01,  8.7458e-01,\n",
      "         1.2986e+00,  8.0378e-01,  9.7514e-01,  9.7813e-01,  8.2485e-01,\n",
      "         7.4854e-01,  6.1338e-01,  5.9496e-01,  7.8757e-01,  8.5395e-01,\n",
      "         8.6280e-01,  7.5089e-01,  9.0887e-01,  1.1476e+00,  1.0873e+00,\n",
      "         7.2777e-01,  7.4729e-01,  5.7484e-01,  1.0145e+00,  8.9294e-01,\n",
      "         7.7855e-01,  8.6505e-01,  7.2196e-01,  9.5526e-01,  8.7862e-01,\n",
      "         1.8137e-01,  7.4810e-01,  7.4602e-01,  3.0640e-01,  6.8740e-01,\n",
      "         1.4278e-03,  9.8463e-02,  5.4639e-01,  6.1884e-02, -2.7154e-01,\n",
      "         2.4213e-01,  2.3587e-01, -4.5867e-02, -2.8904e-01,  1.5456e-01,\n",
      "         3.6020e-01, -2.4376e-01, -1.0357e-01, -7.0327e-01, -1.7468e-01,\n",
      "        -7.5030e-02,  1.9776e-01, -6.9596e-01,  4.2823e-01, -2.7109e-01,\n",
      "         1.8006e-01,  8.0833e-02,  1.6613e-01,  4.3513e-01,  1.2612e-01,\n",
      "        -1.2823e-02, -5.1390e-01,  1.6000e-01, -1.9023e-01, -4.5535e-02,\n",
      "         2.7352e-01,  3.0720e-01, -3.0515e-01, -3.9164e-01, -2.0079e-01,\n",
      "        -5.7885e-01, -2.8800e-01, -1.8181e-01, -4.7485e-01, -5.6045e-01,\n",
      "         3.7940e-01,  3.8337e-01, -3.0136e-01, -6.9791e-02,  1.7122e-01,\n",
      "        -3.2325e-02,  6.8370e-02,  2.6556e-01,  1.2171e-01, -5.6097e-01,\n",
      "         4.7462e-01, -2.0043e-01, -1.9352e-01,  3.8693e-01,  2.0798e-01,\n",
      "        -1.3111e-01,  2.4645e-01,  4.8275e-01, -3.1942e-01, -3.1366e-01,\n",
      "         3.1959e-01, -8.2945e-01,  6.0894e-02, -2.6709e-01, -7.0336e-02,\n",
      "         8.1490e-02, -6.5751e-02,  3.5728e-01, -5.6530e-03,  4.1118e-01,\n",
      "        -2.3882e-01, -1.8672e-01, -2.1833e-01, -5.1440e-02, -8.1007e-03,\n",
      "         4.8670e-01, -2.8268e-02,  3.0911e-01,  1.5644e-01,  1.0762e-01,\n",
      "         3.9918e-01,  9.9574e-02,  2.7025e-02,  7.3850e-01, -8.5950e-01,\n",
      "        -6.1311e-01, -9.8304e-02,  2.1872e-01, -8.4889e-02,  3.8473e-01,\n",
      "         1.9260e-01,  6.5761e-01,  4.0395e-01, -7.1151e-01, -2.8296e-01,\n",
      "        -2.8526e-01, -2.7861e-01, -3.7150e-01,  3.1986e-01,  6.0759e-01,\n",
      "         4.5411e-01,  1.3383e-01,  1.9350e-01,  3.6184e-02,  3.3120e-01,\n",
      "         3.5278e-01, -3.9608e-01, -5.2399e-01, -1.0331e-01, -2.6657e-01,\n",
      "         2.1334e-01, -1.8702e-01,  4.9889e-02,  1.6232e-01, -3.5800e-01,\n",
      "         3.4583e-02,  4.3510e-02, -2.4906e-01, -2.7981e-01,  1.3931e-01,\n",
      "        -5.5007e-01, -4.6789e-01, -1.9660e-01, -2.6802e-01,  3.9671e-02,\n",
      "         2.8194e-01,  6.5404e-01, -1.4117e-01, -3.1999e-01, -1.2291e-02,\n",
      "         2.2953e-01,  1.8556e-01,  5.0388e-01,  6.1848e-01, -4.9514e-01,\n",
      "        -3.6202e-01, -1.0448e-01,  4.5127e-01, -3.0947e-01, -1.7481e-01,\n",
      "         1.6442e-02,  4.1746e-01, -6.6682e-01,  5.0309e-01, -2.6276e-01,\n",
      "        -5.2682e-01, -1.2662e-01, -7.1933e-01,  2.6989e-01,  5.7127e-01,\n",
      "        -4.9057e-01,  5.7128e-03,  6.5044e-01,  1.5832e-01, -3.9113e-01,\n",
      "         9.3375e-02, -5.4186e-01, -4.3874e-02, -7.4900e-02, -4.2908e-02,\n",
      "         7.0391e-02,  1.0723e-01, -5.4150e-02, -2.3840e-01, -6.0680e-02,\n",
      "        -7.2016e-02, -1.1598e-01, -3.0908e-01, -1.6452e-01, -1.1974e-01,\n",
      "         1.8345e-01,  9.4208e-02, -2.9529e-01,  2.0711e-02, -5.7070e-01,\n",
      "         2.4096e-01,  8.5671e-02,  4.8009e-01, -1.2553e-01,  3.0733e-01,\n",
      "         3.7943e-01, -2.8814e-02, -2.0238e-01, -1.0263e-01, -4.4182e-01,\n",
      "         7.8051e-01,  6.2551e-01,  2.7812e-02, -1.6324e-01,  2.3333e-02,\n",
      "         1.9665e-01, -1.3114e-01, -1.6127e-02, -7.5951e-02, -7.1420e-01,\n",
      "        -2.0077e-02,  1.9682e-01,  3.1428e-01, -1.1866e-02,  3.3046e-01,\n",
      "        -6.0140e-01, -3.0346e-01, -4.1196e-01, -2.5876e-01, -1.2890e-01,\n",
      "        -5.0165e-01, -3.4487e-01, -2.5679e-01, -2.5510e-01, -3.1141e-01,\n",
      "        -9.6256e-01, -1.6112e-01,  2.7624e-02, -1.0790e-01, -3.1275e-01,\n",
      "        -4.4154e-01, -1.7567e-01, -1.5342e-01, -3.9169e-01, -6.7650e-01,\n",
      "        -5.0304e-01, -1.9964e-02, -2.5868e-01, -3.1179e-01, -2.0838e-01,\n",
      "        -5.8770e-01, -3.9910e-01, -3.5618e-01, -4.9050e-01, -2.9422e-01,\n",
      "        -2.4247e-01, -2.8429e-01, -3.6677e-02, -2.2885e-01, -5.6926e-01,\n",
      "        -2.9966e-01, -1.2426e-01, -3.0807e-01, -2.8464e-01, -4.3549e-01,\n",
      "        -6.9063e-01, -3.4259e-01, -5.6359e-01, -2.4922e-01, -5.0297e-01,\n",
      "        -4.3372e-01, -3.6401e-01, -1.2139e-01, -1.3983e-01, -2.0886e-01,\n",
      "        -4.1179e-01, -4.8034e-01, -3.4580e-01, -5.1785e-01, -1.5049e-01,\n",
      "        -1.6592e-01, -3.1400e-01, -9.5982e-01, -1.2847e-01, -2.0299e-01,\n",
      "        -7.6757e-01, -2.6631e-01, -2.3318e-01, -1.2092e-01, -1.5641e-01,\n",
      "        -6.7965e-01, -3.4488e-01, -5.3538e-01, -7.4866e-02, -2.7088e-01,\n",
      "        -3.8774e-01, -4.4732e-01, -2.4539e-01, -6.5708e-01, -3.7782e-01,\n",
      "        -3.6176e-01, -3.5715e-02, -8.6624e-02, -1.0403e-01, -3.0789e-01,\n",
      "        -7.6411e-01, -4.6061e-01, -3.6659e-01, -4.1470e-01, -5.4956e-01,\n",
      "        -6.6727e-02, -3.1873e-01, -2.6416e-01, -3.8453e-01, -1.5477e-01,\n",
      "        -5.5524e-01, -2.5042e-01, -5.6192e-01, -5.6467e-01, -3.2564e-01,\n",
      "        -1.9211e-01, -4.3016e-01, -5.0237e-01, -3.6818e-01, -4.0939e-01,\n",
      "        -3.5111e-01, -2.9284e-01, -4.0926e-01, -1.7565e-01, -6.1417e-01,\n",
      "        -2.8147e-02, -2.2463e-01, -1.4299e-01, -3.6546e-01, -5.5246e-01,\n",
      "        -2.2649e-01, -5.4433e-01, -6.5060e-01, -6.8854e-01, -1.6636e-01,\n",
      "        -1.0364e-01, -7.8424e-01, -4.5118e-01, -4.2304e-01, -6.6483e-01,\n",
      "        -2.6467e-01, -6.1959e-02, -2.6587e-01, -2.1531e-01, -4.8208e-01,\n",
      "        -5.9461e-01, -7.5356e-01, -3.1838e-01, -4.3683e-01, -3.5605e-01,\n",
      "        -4.2825e-01, -1.6073e-01, -3.9383e-01, -2.3532e-01, -4.8675e-01,\n",
      "        -4.0493e-01, -3.4288e-01, -3.1979e-01, -1.9949e-01, -4.0583e-01,\n",
      "        -4.6598e-01, -3.7904e-01, -8.3139e-01, -7.3022e-01, -2.8643e-01,\n",
      "        -3.2406e-01, -2.7547e-01, -5.1462e-01, -3.4213e-01, -3.7178e-01,\n",
      "        -4.9805e-01, -1.0484e-01, -1.0608e+00, -3.8095e-01, -3.9614e-01,\n",
      "        -4.4622e-01, -3.3181e-01, -5.6958e-01, -5.4431e-01, -1.5188e-01,\n",
      "        -3.1286e-01, -4.0539e-01, -7.5996e-01, -2.4477e-01, -7.9193e-01,\n",
      "        -4.4644e-02, -2.5442e-01, -4.9655e-01, -3.0449e-01, -2.5090e-01,\n",
      "        -8.9006e-02, -3.3069e-01, -5.9929e-01, -3.3034e-01, -4.4092e-01,\n",
      "        -4.0550e-01, -1.8025e-01, -3.6270e-01, -3.3770e-01, -4.7527e-01,\n",
      "        -3.6574e-01, -5.4066e-01, -3.4407e-01, -8.4674e-01, -7.0090e-01,\n",
      "        -5.8012e-01, -4.0686e-01, -4.0257e-01, -4.4235e-01, -2.7991e-01,\n",
      "        -7.5184e-01, -5.8519e-01, -2.3481e-01, -4.4350e-01, -2.2110e-01,\n",
      "        -1.7259e-01, -2.2409e-01,  1.8470e-01, -1.8290e-01, -6.1163e-01],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2908, -0.0276, -0.0818,  ...,  0.2182, -0.2045,  0.1735],\n",
      "        [-0.0739, -0.1650, -0.1403,  ..., -0.1643,  0.0082,  0.0358],\n",
      "        [-0.0318, -0.2122, -0.0470,  ..., -0.1917, -0.0893,  0.2005],\n",
      "        ...,\n",
      "        [-0.1362,  0.0470,  0.0042,  ..., -0.0206, -0.1882,  0.1381],\n",
      "        [ 0.1753,  0.2109, -0.0095,  ..., -0.0130,  0.0380, -0.0684],\n",
      "        [ 0.3284, -0.2590,  0.3550,  ...,  0.0626,  0.3527,  0.0041]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0599, -0.1995, -0.0314,  ..., -0.0884, -0.1112,  0.0175],\n",
      "        [ 0.0244,  0.0294,  0.0607,  ..., -0.0154, -0.0436, -0.0891],\n",
      "        [ 0.0469, -0.0417, -0.0649,  ...,  0.0927, -0.0715, -0.0750],\n",
      "        ...,\n",
      "        [-0.0092,  0.0459,  0.0977,  ..., -0.0848, -0.0274,  0.1073],\n",
      "        [-0.0037,  0.2237, -0.1335,  ..., -0.1314,  0.0295, -0.1133],\n",
      "        [-0.0142, -0.0074,  0.0477,  ..., -0.1277, -0.0791, -0.0879]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-1.9896e-01,  1.8369e-02, -2.2527e-01, -5.2473e-02, -1.5172e-01,\n",
      "        -1.0098e-01,  3.4251e-01, -2.1980e-01,  1.1337e-01, -2.9983e-01,\n",
      "        -3.3327e-02,  6.4523e-02, -2.2425e-01, -2.3871e-01, -3.5945e-01,\n",
      "        -1.8607e-01, -3.2907e-01, -1.6006e-01, -2.6414e-01,  2.6516e-01,\n",
      "         5.7698e-02, -1.0960e-01, -3.0711e-01, -2.0677e-01, -4.2880e-01,\n",
      "         7.9075e-01, -3.2605e-01, -1.9335e-01, -2.6332e-02,  1.0145e-03,\n",
      "        -5.3870e-01, -1.8940e-01, -2.1924e-01, -2.7723e-01,  1.1886e-01,\n",
      "        -1.8964e-01,  4.3377e-02, -1.1498e-01, -1.5353e-01, -2.8008e-01,\n",
      "        -2.3221e-01, -3.5230e-01, -5.5946e-01,  1.6020e-02, -2.6189e-01,\n",
      "        -1.2601e-01, -3.6639e-01, -5.0355e-01,  1.5306e-02, -3.0709e-02,\n",
      "        -2.1539e-02, -4.9473e-04,  1.8970e-01,  7.7198e-02, -3.4139e-01,\n",
      "         5.9501e-02, -2.1627e-01, -6.6607e-02, -1.1857e-01,  1.5158e-01,\n",
      "        -3.9161e-02, -1.5314e-01, -1.9150e-01, -1.1720e-01, -6.2817e-01,\n",
      "         3.9731e-02, -4.5391e-01, -3.3520e-01, -1.5810e-01, -2.9537e-01,\n",
      "        -3.9422e-01, -1.4907e-01,  3.2461e-01, -1.0026e-01, -2.5960e-01,\n",
      "        -3.8294e-01, -2.2920e-01,  9.0696e-02, -2.4578e-01, -5.9329e-02,\n",
      "        -8.0189e-02, -1.7373e-01,  2.2431e-02,  3.2189e-02, -1.3043e-01,\n",
      "        -1.6339e-01, -4.9879e-01, -1.9923e-01, -1.7530e-01, -4.7484e-01,\n",
      "         3.8930e-02, -1.7264e-01, -1.4657e-01,  3.4724e-02, -3.0136e-01,\n",
      "        -2.5719e-01, -2.4872e-01, -1.5145e-01, -2.5372e-01, -3.0456e-02,\n",
      "         4.9526e-02, -2.5814e-01, -1.9344e-01, -1.1793e-01,  1.5088e-01,\n",
      "        -4.9461e-01, -2.3385e-01, -6.1891e-02, -1.3657e-01, -1.4167e-01,\n",
      "        -9.1902e-02, -3.7972e-01, -2.9638e-01, -1.8590e-01, -3.6213e-01,\n",
      "        -3.7456e-01, -1.7834e-01, -1.4677e-01,  7.8372e-01, -1.6971e-01,\n",
      "        -8.9674e-02, -1.8611e-01, -3.7942e-01, -5.0451e-01, -7.8538e-03,\n",
      "         3.3895e-01,  2.9200e-02, -4.9196e-01,  1.3889e-02,  1.5517e-01,\n",
      "        -6.9899e-02, -4.2017e-01, -2.9592e-01, -1.9268e-01, -2.5847e-01,\n",
      "        -5.5769e-01, -3.8330e-01, -2.8727e-01,  1.1908e-01,  3.9593e-02,\n",
      "         1.8386e-01, -1.9904e-01, -2.9069e-01, -1.8878e-01,  3.8673e-02,\n",
      "        -4.8185e-02, -3.2858e-01, -1.7480e-01, -1.7815e-01, -4.4715e-01,\n",
      "        -2.8563e-01, -1.3575e-01, -2.1518e-01, -1.6525e-01, -3.6947e-01,\n",
      "        -8.9412e-02,  1.0642e-01,  1.7248e-02, -2.2769e-01, -4.6251e-01,\n",
      "         2.2500e-02,  1.5629e-02, -3.0083e-01,  1.2935e-02, -1.6101e-01,\n",
      "        -2.8683e-01, -6.6313e-02, -6.2796e-01, -3.1640e-01, -8.6305e-03,\n",
      "        -1.5406e-01, -1.5611e-01, -2.9559e-01,  1.1849e-01, -3.0692e-01,\n",
      "         2.2427e-01, -1.0684e-01, -1.9681e-01, -9.4544e-02, -1.4919e-01,\n",
      "        -4.8880e-01,  1.1546e-01, -1.4402e-01, -2.9138e-01, -3.5639e-01,\n",
      "        -2.1697e-01, -6.3271e-02, -1.6285e-01, -1.6655e-02, -1.8518e-01,\n",
      "        -5.1947e-02, -1.4256e-01, -1.9332e-01, -1.4899e-01, -9.7500e-02,\n",
      "        -9.1662e-02, -1.5321e-01, -2.6259e-01, -6.0016e-02, -1.3320e-01,\n",
      "         9.3918e-01,  7.3398e-01,  7.5447e-01,  6.7737e-01,  1.0469e+00,\n",
      "         6.5638e-01,  9.4282e-01,  7.9206e-01,  9.3561e-01,  8.5828e-01,\n",
      "         9.9323e-01,  9.8407e-01,  9.1339e-01,  9.7631e-01,  5.6458e-01,\n",
      "         8.1126e-01,  8.7394e-01,  5.4464e-01,  6.6834e-01,  6.2161e-01,\n",
      "         9.7287e-01,  6.7531e-01,  6.6469e-01,  1.0316e+00,  6.7163e-01,\n",
      "         3.0720e-01,  5.5119e-01,  7.1411e-01,  9.5081e-01,  7.8563e-01,\n",
      "         6.5416e-01,  8.7328e-01,  9.9064e-01,  8.6779e-01,  8.3180e-01,\n",
      "         7.2511e-01,  8.0176e-01,  9.5817e-01,  7.0706e-01,  7.5118e-01,\n",
      "         9.4980e-01,  6.9175e-01,  6.6527e-01,  8.0093e-01,  7.6502e-01,\n",
      "         1.1733e+00,  6.2396e-01,  6.9372e-01,  1.0135e+00,  6.9557e-01,\n",
      "         1.0030e+00,  8.8550e-01,  9.1929e-01,  1.1718e+00,  8.6983e-01,\n",
      "         1.0204e+00,  8.2363e-01,  9.1782e-01,  8.5432e-01,  7.8958e-01,\n",
      "         1.1277e+00,  9.1619e-01,  7.8748e-01,  8.3618e-01,  8.1763e-01,\n",
      "         1.0835e+00,  6.3058e-01,  5.6078e-01,  5.3678e-01,  9.2057e-01,\n",
      "         1.0615e+00,  8.8393e-01,  9.7201e-01,  7.0617e-01,  8.0260e-01,\n",
      "         8.1166e-01,  1.1030e+00,  1.0172e+00,  7.5380e-01,  8.3310e-01,\n",
      "         8.9010e-01,  6.1083e-01,  9.6983e-01,  1.0104e+00,  8.4516e-01,\n",
      "         6.6179e-01,  8.4092e-01,  9.2698e-01,  8.5520e-01,  8.9326e-01,\n",
      "         1.0213e+00,  8.1179e-01,  8.4952e-01,  7.7478e-01,  8.6351e-01,\n",
      "         8.7398e-01,  8.4105e-01,  8.1541e-01,  9.7695e-01,  1.0308e+00,\n",
      "         9.3773e-01,  8.6443e-01,  8.8014e-01,  8.5930e-01,  9.3873e-01,\n",
      "         9.1464e-01,  5.0773e-01,  1.0329e+00,  8.1886e-01,  1.0364e+00,\n",
      "         7.6979e-01,  4.9450e-01,  5.6590e-01,  9.2599e-01,  8.0686e-01,\n",
      "         9.8525e-01,  1.0959e+00,  6.7511e-01,  3.4648e-01,  1.1621e+00,\n",
      "         1.1016e+00,  9.0340e-01,  8.7755e-01,  7.1922e-01,  5.6653e-01,\n",
      "         9.6154e-01,  7.6723e-01,  4.7492e-01,  7.8152e-01,  5.2247e-01,\n",
      "         9.1791e-01,  6.8404e-01,  7.1519e-01,  7.2652e-01,  6.7508e-01,\n",
      "         7.5670e-01,  7.7175e-01,  1.1177e+00,  7.2913e-01,  1.0823e+00,\n",
      "         1.0571e+00,  8.1947e-01,  7.0635e-01,  6.8277e-01,  8.4363e-01,\n",
      "         8.9947e-01,  8.3680e-01,  8.0369e-01,  9.4233e-01,  6.5832e-01,\n",
      "         7.3807e-01,  8.1330e-01,  7.4173e-01,  1.0347e+00,  6.7555e-01,\n",
      "         8.5692e-01,  8.9837e-01,  8.4423e-01,  7.0895e-01,  8.1038e-01,\n",
      "         9.7485e-01,  7.6763e-01,  6.8482e-01,  1.0183e+00,  1.0070e+00,\n",
      "         8.5227e-01,  9.3016e-01,  7.7012e-01,  7.6593e-01,  2.0603e-01,\n",
      "         8.3610e-01,  6.7190e-01,  9.9488e-01,  8.8057e-01,  6.7273e-01,\n",
      "         6.2946e-01,  9.6138e-01,  8.7019e-01,  4.4365e-01,  6.1485e-01,\n",
      "         8.0132e-01,  3.6240e-01,  9.6942e-01,  7.7164e-01,  9.1225e-01,\n",
      "         6.7765e-01,  7.0889e-01,  8.7015e-01,  9.6485e-01,  7.9620e-01,\n",
      "         9.4450e-01,  8.4790e-01,  7.9035e-01,  3.2379e-01,  8.2271e-01,\n",
      "         9.0980e-01,  7.8183e-01,  8.2739e-01,  7.5973e-01,  8.7510e-01,\n",
      "         8.3817e-02,  3.5632e-01,  1.1361e-02, -5.2795e-02, -5.0247e-02,\n",
      "         1.9359e-01,  3.2350e-01,  1.6940e-01,  6.7344e-02, -4.5761e-01,\n",
      "         4.1874e-01,  9.0177e-02, -5.3819e-01, -2.0479e-01,  1.2132e-01,\n",
      "        -1.2972e-01, -2.1275e-01,  4.6752e-01,  2.5378e-02,  1.0040e-02,\n",
      "         1.4136e-01,  2.2415e-01, -1.9154e-01,  1.5471e-01, -1.8911e-01,\n",
      "         4.2661e-01, -1.3541e-01,  7.9177e-02, -2.3534e-01,  2.6827e-01,\n",
      "        -1.9696e-01, -1.0579e-01,  6.1323e-02,  9.4436e-02, -5.3750e-01,\n",
      "        -3.9430e-01,  3.5863e-01, -4.7279e-01,  2.7479e-01, -1.9127e-02,\n",
      "        -1.3765e-01, -2.2391e-01, -5.7244e-02,  2.8265e-01,  9.1179e-02,\n",
      "        -1.6974e-01, -3.6563e-03, -3.0378e-01,  1.0275e-01,  2.8603e-01,\n",
      "         1.5284e-01, -1.1496e-01, -6.6723e-02, -3.8780e-01,  1.1502e-01,\n",
      "         1.0204e-01,  2.7751e-02,  4.6031e-01, -5.9107e-02,  3.5761e-01,\n",
      "         1.4141e-01,  8.9702e-02, -1.1526e-02,  4.0679e-02,  4.1622e-02,\n",
      "        -4.4368e-01, -5.8494e-02, -8.6245e-02,  5.7261e-02, -2.0011e-01,\n",
      "         5.0371e-01, -1.9921e-02,  4.2518e-01,  1.4932e-01,  1.9169e-02,\n",
      "         2.6264e-01, -4.4779e-01, -1.2438e-01, -2.2706e-01, -1.4734e-01,\n",
      "         1.9828e-01,  4.7701e-02, -5.1628e-01, -4.5076e-01, -2.4734e-02,\n",
      "         6.9974e-03, -3.1548e-01, -3.1729e-02, -4.0810e-01,  1.1894e-01,\n",
      "        -1.7984e-01,  2.2757e-01,  6.8239e-03, -4.5536e-02,  2.6821e-01,\n",
      "        -2.5631e-01,  6.8308e-02,  9.5123e-02,  2.3099e-01, -5.3566e-02,\n",
      "        -8.5157e-02, -1.0293e-01, -2.3608e-01,  2.3038e-01,  3.4019e-01,\n",
      "         1.7008e-01, -1.3069e-01, -1.0210e-01, -1.2557e-01,  2.1915e-01,\n",
      "        -1.0970e-01, -3.8893e-02, -1.3000e-02, -3.0647e-01, -1.7798e-01,\n",
      "        -1.2460e-01, -8.4682e-02,  8.5718e-02,  5.1149e-01,  2.1060e-01,\n",
      "        -2.3976e-01,  5.4610e-01,  2.8269e-01, -1.2243e-01, -1.5660e-01,\n",
      "         5.9204e-01, -1.3112e-01, -3.0173e-01, -2.3261e-02, -8.2783e-02,\n",
      "         4.0307e-01, -3.6021e-01,  4.8660e-01, -1.0227e-01, -4.0129e-01,\n",
      "         4.4397e-02,  3.2855e-01, -1.4878e-01, -4.9653e-02,  1.7602e-01,\n",
      "         2.9271e-01, -1.0973e-01, -2.1147e-01, -3.0712e-01, -1.8844e-01,\n",
      "         3.4860e-01, -3.8962e-02, -1.0880e-01, -3.5822e-01,  5.5714e-01,\n",
      "         2.2493e-01, -2.9192e-02, -1.7401e-01,  1.4881e-01,  8.5501e-02,\n",
      "        -2.0139e-01, -8.6202e-02,  2.3609e-01,  8.6516e-03,  1.9856e-01,\n",
      "        -2.2481e-01,  3.0018e-02, -1.1444e-01, -1.9980e-01,  7.0368e-01,\n",
      "         2.1352e-01,  2.6958e-01,  1.7594e-01,  1.8089e-01,  2.2366e-01,\n",
      "         2.5850e-01,  1.5830e-01, -3.5176e-01, -1.2989e-01,  1.4032e-01,\n",
      "         1.7093e-01,  2.5160e-01, -5.1221e-02,  7.9565e-02, -1.9103e-01,\n",
      "        -4.1595e-02, -2.8131e-01, -2.1766e-01,  9.7148e-02,  1.9721e-02,\n",
      "         1.8985e-02,  5.0055e-02,  2.4437e-01,  3.4789e-01, -2.2564e-02,\n",
      "         3.0529e-01,  1.1589e-01, -2.3206e-01, -1.8445e-01,  1.2727e-01,\n",
      "        -1.5138e-02,  2.0443e-01,  6.3982e-03, -1.2455e-01,  1.3756e-02,\n",
      "        -3.4413e-01, -3.2208e-01, -2.7928e-01, -3.3388e-01, -3.7074e-01,\n",
      "        -4.4049e-02, -6.1460e-01, -4.2605e-01, -9.2067e-02, -3.5830e-01,\n",
      "        -3.9654e-01, -2.6005e-01, -2.2367e-01, -4.2566e-01, -5.5925e-01,\n",
      "        -6.0508e-01, -2.7200e-01, -4.3747e-01, -5.8627e-01, -2.4428e-01,\n",
      "        -3.9004e-02, -5.3268e-01, -4.3958e-01, -8.5982e-01, -3.9454e-01,\n",
      "         1.1262e-01, -4.7654e-01, -4.1132e-01, -2.2667e-01, -6.8753e-01,\n",
      "        -4.0940e-01, -2.6463e-01, -1.6872e-01, -3.3353e-01, -1.0537e-01,\n",
      "        -1.5291e-01, -1.9922e-01, -4.9909e-01, -6.3670e-01, -4.2671e-01,\n",
      "        -2.5785e-01, -2.3476e-01, -3.5805e-01, -2.1716e-01, -1.6483e-01,\n",
      "        -1.2136e-01, -7.3447e-01,  6.9531e-02, -7.3833e-01, -5.3146e-01,\n",
      "        -2.9547e-01, -2.3935e-01, -1.1098e+00, -6.6346e-01, -4.5489e-01,\n",
      "        -6.1067e-01, -4.2938e-01, -4.2522e-01, -3.9158e-01, -4.3628e-01,\n",
      "        -1.4175e-01, -6.7278e-01, -6.1650e-01, -3.3496e-01, -3.4610e-01,\n",
      "        -3.5793e-01, -6.0118e-01, -5.4646e-01, -3.4308e-01, -2.5622e-01,\n",
      "        -3.7956e-01, -2.2628e-01, -8.2648e-01,  1.4650e-02, -5.3926e-01,\n",
      "        -5.8411e-01, -4.1610e-01, -4.7468e-01, -3.2089e-01, -4.7680e-01,\n",
      "        -1.9381e-01, -4.5869e-01, -1.8284e-01, -4.8120e-01, -2.6030e-01,\n",
      "        -4.1996e-01, -2.9099e-01, -2.4501e-01, -1.1521e-01, -3.8876e-01,\n",
      "        -6.7073e-01, -2.6060e-01, -3.1124e-01, -4.8611e-01, -2.2364e-01,\n",
      "        -6.2944e-01, -2.3952e-01, -4.6483e-01, -5.5043e-01, -4.4714e-01,\n",
      "        -4.3714e-01, -4.5892e-01, -2.6370e-01, -3.1202e-01, -6.2937e-01,\n",
      "        -2.6673e-01, -1.4085e-01, -3.6818e-01, -2.5663e-01, -3.0573e-01,\n",
      "        -4.1798e-01, -1.1895e-01, -4.8087e-01, -3.8435e-01, -3.4743e-01,\n",
      "        -4.2075e-01, -8.7884e-01, -3.4277e-01,  1.7818e-01, -5.9937e-01,\n",
      "        -5.2748e-01, -4.5096e-01, -6.7230e-01, -3.9579e-01, -7.0562e-01,\n",
      "        -3.0879e-01, -7.2413e-02, -4.1246e-01, -4.3837e-01, -9.1857e-01,\n",
      "        -8.9224e-01, -8.6363e-01, -1.1391e+00, -2.5436e-01, -4.6521e-01,\n",
      "        -4.8936e-01, -6.4422e-01, -1.2768e+00, -2.8899e-01, -5.0160e-01,\n",
      "        -3.5809e-01, -3.0336e-01, -2.3515e-01, -1.3725e-01, -7.5674e-01,\n",
      "        -4.0306e-01, -6.6200e-01, -2.6380e-01, -1.7746e-01, -5.5275e-01,\n",
      "        -7.4601e-01, -4.4341e-01, -2.5738e-01, -3.5780e-01, -4.1272e-01,\n",
      "        -2.2464e-01, -1.6649e-01, -2.6113e-01, -4.3488e-01, -3.5676e-01,\n",
      "        -2.5601e-02, -2.7972e-01, -2.7931e-01, -4.8900e-01, -4.0714e-01,\n",
      "        -4.4893e-01, -2.8465e-01, -4.0615e-01, -5.8578e-01, -1.4813e-01,\n",
      "        -2.3743e-01, -4.3030e-01, -1.6055e-01, -2.5200e-01, -5.1946e-02,\n",
      "        -1.8946e-01, -5.9929e-01, -1.0837e-01, -5.5855e-01, -8.8452e-02,\n",
      "        -3.2613e-01,  1.7746e-01, -7.4220e-02, -2.8766e-01, -8.4513e-01,\n",
      "        -4.0479e-01, -4.8621e-01, -4.1223e-01, -1.5384e-01, -4.7901e-01,\n",
      "        -6.5176e-01, -4.7152e-01, -2.1000e-01, -7.4626e-01, -5.9215e-01,\n",
      "        -4.6874e-01, -3.8679e-01, -3.5116e-01, -7.5207e-01, -7.2872e-01],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-1.9896e-01,  1.8369e-02, -2.2527e-01, -5.2473e-02, -1.5172e-01,\n",
      "        -1.0098e-01,  3.4251e-01, -2.1980e-01,  1.1337e-01, -2.9983e-01,\n",
      "        -3.3327e-02,  6.4523e-02, -2.2425e-01, -2.3871e-01, -3.5945e-01,\n",
      "        -1.8607e-01, -3.2907e-01, -1.6006e-01, -2.6414e-01,  2.6516e-01,\n",
      "         5.7698e-02, -1.0960e-01, -3.0711e-01, -2.0677e-01, -4.2880e-01,\n",
      "         7.9075e-01, -3.2605e-01, -1.9335e-01, -2.6332e-02,  1.0145e-03,\n",
      "        -5.3870e-01, -1.8940e-01, -2.1924e-01, -2.7723e-01,  1.1886e-01,\n",
      "        -1.8964e-01,  4.3377e-02, -1.1498e-01, -1.5353e-01, -2.8008e-01,\n",
      "        -2.3221e-01, -3.5230e-01, -5.5946e-01,  1.6020e-02, -2.6189e-01,\n",
      "        -1.2601e-01, -3.6639e-01, -5.0355e-01,  1.5306e-02, -3.0709e-02,\n",
      "        -2.1539e-02, -4.9473e-04,  1.8970e-01,  7.7198e-02, -3.4139e-01,\n",
      "         5.9501e-02, -2.1627e-01, -6.6607e-02, -1.1857e-01,  1.5158e-01,\n",
      "        -3.9161e-02, -1.5314e-01, -1.9150e-01, -1.1720e-01, -6.2817e-01,\n",
      "         3.9731e-02, -4.5391e-01, -3.3520e-01, -1.5810e-01, -2.9537e-01,\n",
      "        -3.9422e-01, -1.4907e-01,  3.2461e-01, -1.0026e-01, -2.5960e-01,\n",
      "        -3.8294e-01, -2.2920e-01,  9.0696e-02, -2.4578e-01, -5.9329e-02,\n",
      "        -8.0189e-02, -1.7373e-01,  2.2431e-02,  3.2189e-02, -1.3043e-01,\n",
      "        -1.6339e-01, -4.9879e-01, -1.9923e-01, -1.7530e-01, -4.7484e-01,\n",
      "         3.8930e-02, -1.7264e-01, -1.4657e-01,  3.4724e-02, -3.0136e-01,\n",
      "        -2.5719e-01, -2.4872e-01, -1.5145e-01, -2.5372e-01, -3.0456e-02,\n",
      "         4.9526e-02, -2.5814e-01, -1.9344e-01, -1.1793e-01,  1.5088e-01,\n",
      "        -4.9461e-01, -2.3385e-01, -6.1891e-02, -1.3657e-01, -1.4167e-01,\n",
      "        -9.1902e-02, -3.7972e-01, -2.9638e-01, -1.8590e-01, -3.6213e-01,\n",
      "        -3.7456e-01, -1.7834e-01, -1.4677e-01,  7.8372e-01, -1.6971e-01,\n",
      "        -8.9674e-02, -1.8611e-01, -3.7942e-01, -5.0451e-01, -7.8538e-03,\n",
      "         3.3895e-01,  2.9200e-02, -4.9196e-01,  1.3889e-02,  1.5517e-01,\n",
      "        -6.9899e-02, -4.2017e-01, -2.9592e-01, -1.9268e-01, -2.5847e-01,\n",
      "        -5.5769e-01, -3.8330e-01, -2.8727e-01,  1.1908e-01,  3.9593e-02,\n",
      "         1.8386e-01, -1.9904e-01, -2.9069e-01, -1.8878e-01,  3.8673e-02,\n",
      "        -4.8185e-02, -3.2858e-01, -1.7480e-01, -1.7815e-01, -4.4715e-01,\n",
      "        -2.8563e-01, -1.3575e-01, -2.1518e-01, -1.6525e-01, -3.6947e-01,\n",
      "        -8.9412e-02,  1.0642e-01,  1.7248e-02, -2.2769e-01, -4.6251e-01,\n",
      "         2.2500e-02,  1.5629e-02, -3.0083e-01,  1.2935e-02, -1.6101e-01,\n",
      "        -2.8683e-01, -6.6313e-02, -6.2796e-01, -3.1640e-01, -8.6305e-03,\n",
      "        -1.5406e-01, -1.5611e-01, -2.9559e-01,  1.1849e-01, -3.0692e-01,\n",
      "         2.2427e-01, -1.0684e-01, -1.9681e-01, -9.4544e-02, -1.4919e-01,\n",
      "        -4.8880e-01,  1.1546e-01, -1.4402e-01, -2.9138e-01, -3.5639e-01,\n",
      "        -2.1697e-01, -6.3271e-02, -1.6285e-01, -1.6655e-02, -1.8518e-01,\n",
      "        -5.1947e-02, -1.4256e-01, -1.9332e-01, -1.4899e-01, -9.7500e-02,\n",
      "        -9.1662e-02, -1.5321e-01, -2.6259e-01, -6.0016e-02, -1.3320e-01,\n",
      "         9.3918e-01,  7.3398e-01,  7.5447e-01,  6.7737e-01,  1.0469e+00,\n",
      "         6.5638e-01,  9.4282e-01,  7.9206e-01,  9.3561e-01,  8.5828e-01,\n",
      "         9.9323e-01,  9.8407e-01,  9.1339e-01,  9.7631e-01,  5.6458e-01,\n",
      "         8.1126e-01,  8.7394e-01,  5.4464e-01,  6.6834e-01,  6.2161e-01,\n",
      "         9.7287e-01,  6.7531e-01,  6.6469e-01,  1.0316e+00,  6.7163e-01,\n",
      "         3.0720e-01,  5.5119e-01,  7.1411e-01,  9.5081e-01,  7.8563e-01,\n",
      "         6.5416e-01,  8.7328e-01,  9.9064e-01,  8.6779e-01,  8.3180e-01,\n",
      "         7.2511e-01,  8.0176e-01,  9.5817e-01,  7.0706e-01,  7.5118e-01,\n",
      "         9.4980e-01,  6.9175e-01,  6.6527e-01,  8.0093e-01,  7.6502e-01,\n",
      "         1.1733e+00,  6.2396e-01,  6.9372e-01,  1.0135e+00,  6.9557e-01,\n",
      "         1.0030e+00,  8.8550e-01,  9.1929e-01,  1.1718e+00,  8.6983e-01,\n",
      "         1.0204e+00,  8.2363e-01,  9.1782e-01,  8.5432e-01,  7.8958e-01,\n",
      "         1.1277e+00,  9.1619e-01,  7.8748e-01,  8.3618e-01,  8.1763e-01,\n",
      "         1.0835e+00,  6.3058e-01,  5.6078e-01,  5.3678e-01,  9.2057e-01,\n",
      "         1.0615e+00,  8.8393e-01,  9.7201e-01,  7.0617e-01,  8.0260e-01,\n",
      "         8.1166e-01,  1.1030e+00,  1.0172e+00,  7.5380e-01,  8.3310e-01,\n",
      "         8.9010e-01,  6.1083e-01,  9.6983e-01,  1.0104e+00,  8.4516e-01,\n",
      "         6.6179e-01,  8.4092e-01,  9.2698e-01,  8.5520e-01,  8.9326e-01,\n",
      "         1.0213e+00,  8.1179e-01,  8.4952e-01,  7.7478e-01,  8.6351e-01,\n",
      "         8.7398e-01,  8.4105e-01,  8.1541e-01,  9.7695e-01,  1.0308e+00,\n",
      "         9.3773e-01,  8.6443e-01,  8.8014e-01,  8.5930e-01,  9.3873e-01,\n",
      "         9.1464e-01,  5.0773e-01,  1.0329e+00,  8.1886e-01,  1.0364e+00,\n",
      "         7.6979e-01,  4.9450e-01,  5.6590e-01,  9.2599e-01,  8.0686e-01,\n",
      "         9.8525e-01,  1.0959e+00,  6.7511e-01,  3.4648e-01,  1.1621e+00,\n",
      "         1.1016e+00,  9.0340e-01,  8.7755e-01,  7.1922e-01,  5.6653e-01,\n",
      "         9.6154e-01,  7.6723e-01,  4.7492e-01,  7.8152e-01,  5.2247e-01,\n",
      "         9.1791e-01,  6.8404e-01,  7.1519e-01,  7.2652e-01,  6.7508e-01,\n",
      "         7.5670e-01,  7.7175e-01,  1.1177e+00,  7.2913e-01,  1.0823e+00,\n",
      "         1.0571e+00,  8.1947e-01,  7.0635e-01,  6.8277e-01,  8.4363e-01,\n",
      "         8.9947e-01,  8.3680e-01,  8.0369e-01,  9.4233e-01,  6.5832e-01,\n",
      "         7.3807e-01,  8.1330e-01,  7.4173e-01,  1.0347e+00,  6.7555e-01,\n",
      "         8.5692e-01,  8.9837e-01,  8.4423e-01,  7.0895e-01,  8.1038e-01,\n",
      "         9.7485e-01,  7.6763e-01,  6.8482e-01,  1.0183e+00,  1.0070e+00,\n",
      "         8.5227e-01,  9.3016e-01,  7.7012e-01,  7.6593e-01,  2.0603e-01,\n",
      "         8.3610e-01,  6.7190e-01,  9.9488e-01,  8.8057e-01,  6.7273e-01,\n",
      "         6.2946e-01,  9.6138e-01,  8.7019e-01,  4.4365e-01,  6.1485e-01,\n",
      "         8.0132e-01,  3.6240e-01,  9.6942e-01,  7.7164e-01,  9.1225e-01,\n",
      "         6.7765e-01,  7.0889e-01,  8.7015e-01,  9.6485e-01,  7.9620e-01,\n",
      "         9.4450e-01,  8.4790e-01,  7.9035e-01,  3.2379e-01,  8.2271e-01,\n",
      "         9.0980e-01,  7.8183e-01,  8.2739e-01,  7.5973e-01,  8.7510e-01,\n",
      "         8.3817e-02,  3.5632e-01,  1.1361e-02, -5.2795e-02, -5.0247e-02,\n",
      "         1.9359e-01,  3.2350e-01,  1.6940e-01,  6.7344e-02, -4.5761e-01,\n",
      "         4.1874e-01,  9.0177e-02, -5.3819e-01, -2.0479e-01,  1.2132e-01,\n",
      "        -1.2972e-01, -2.1275e-01,  4.6752e-01,  2.5378e-02,  1.0040e-02,\n",
      "         1.4136e-01,  2.2415e-01, -1.9154e-01,  1.5471e-01, -1.8911e-01,\n",
      "         4.2661e-01, -1.3541e-01,  7.9177e-02, -2.3534e-01,  2.6827e-01,\n",
      "        -1.9696e-01, -1.0579e-01,  6.1323e-02,  9.4436e-02, -5.3750e-01,\n",
      "        -3.9430e-01,  3.5863e-01, -4.7279e-01,  2.7479e-01, -1.9127e-02,\n",
      "        -1.3765e-01, -2.2391e-01, -5.7244e-02,  2.8265e-01,  9.1179e-02,\n",
      "        -1.6974e-01, -3.6563e-03, -3.0378e-01,  1.0275e-01,  2.8603e-01,\n",
      "         1.5284e-01, -1.1496e-01, -6.6723e-02, -3.8780e-01,  1.1502e-01,\n",
      "         1.0204e-01,  2.7751e-02,  4.6031e-01, -5.9107e-02,  3.5761e-01,\n",
      "         1.4141e-01,  8.9702e-02, -1.1526e-02,  4.0679e-02,  4.1622e-02,\n",
      "        -4.4368e-01, -5.8494e-02, -8.6245e-02,  5.7261e-02, -2.0011e-01,\n",
      "         5.0371e-01, -1.9921e-02,  4.2518e-01,  1.4932e-01,  1.9169e-02,\n",
      "         2.6264e-01, -4.4779e-01, -1.2438e-01, -2.2706e-01, -1.4734e-01,\n",
      "         1.9828e-01,  4.7701e-02, -5.1628e-01, -4.5076e-01, -2.4734e-02,\n",
      "         6.9974e-03, -3.1548e-01, -3.1729e-02, -4.0810e-01,  1.1894e-01,\n",
      "        -1.7984e-01,  2.2757e-01,  6.8239e-03, -4.5536e-02,  2.6821e-01,\n",
      "        -2.5631e-01,  6.8308e-02,  9.5123e-02,  2.3099e-01, -5.3566e-02,\n",
      "        -8.5157e-02, -1.0293e-01, -2.3608e-01,  2.3038e-01,  3.4019e-01,\n",
      "         1.7008e-01, -1.3069e-01, -1.0210e-01, -1.2557e-01,  2.1915e-01,\n",
      "        -1.0970e-01, -3.8893e-02, -1.3000e-02, -3.0647e-01, -1.7798e-01,\n",
      "        -1.2460e-01, -8.4682e-02,  8.5718e-02,  5.1149e-01,  2.1060e-01,\n",
      "        -2.3976e-01,  5.4610e-01,  2.8269e-01, -1.2243e-01, -1.5660e-01,\n",
      "         5.9204e-01, -1.3112e-01, -3.0173e-01, -2.3261e-02, -8.2783e-02,\n",
      "         4.0307e-01, -3.6021e-01,  4.8660e-01, -1.0227e-01, -4.0129e-01,\n",
      "         4.4397e-02,  3.2855e-01, -1.4878e-01, -4.9653e-02,  1.7602e-01,\n",
      "         2.9271e-01, -1.0973e-01, -2.1147e-01, -3.0712e-01, -1.8844e-01,\n",
      "         3.4860e-01, -3.8962e-02, -1.0880e-01, -3.5822e-01,  5.5714e-01,\n",
      "         2.2493e-01, -2.9192e-02, -1.7401e-01,  1.4881e-01,  8.5501e-02,\n",
      "        -2.0139e-01, -8.6202e-02,  2.3609e-01,  8.6516e-03,  1.9856e-01,\n",
      "        -2.2481e-01,  3.0018e-02, -1.1444e-01, -1.9980e-01,  7.0368e-01,\n",
      "         2.1352e-01,  2.6958e-01,  1.7594e-01,  1.8089e-01,  2.2366e-01,\n",
      "         2.5850e-01,  1.5830e-01, -3.5176e-01, -1.2989e-01,  1.4032e-01,\n",
      "         1.7093e-01,  2.5160e-01, -5.1221e-02,  7.9565e-02, -1.9103e-01,\n",
      "        -4.1595e-02, -2.8131e-01, -2.1766e-01,  9.7148e-02,  1.9721e-02,\n",
      "         1.8985e-02,  5.0055e-02,  2.4437e-01,  3.4789e-01, -2.2564e-02,\n",
      "         3.0529e-01,  1.1589e-01, -2.3206e-01, -1.8445e-01,  1.2727e-01,\n",
      "        -1.5138e-02,  2.0443e-01,  6.3982e-03, -1.2455e-01,  1.3756e-02,\n",
      "        -3.4413e-01, -3.2208e-01, -2.7928e-01, -3.3388e-01, -3.7074e-01,\n",
      "        -4.4049e-02, -6.1460e-01, -4.2605e-01, -9.2067e-02, -3.5830e-01,\n",
      "        -3.9654e-01, -2.6005e-01, -2.2367e-01, -4.2566e-01, -5.5925e-01,\n",
      "        -6.0508e-01, -2.7200e-01, -4.3747e-01, -5.8627e-01, -2.4428e-01,\n",
      "        -3.9004e-02, -5.3268e-01, -4.3958e-01, -8.5982e-01, -3.9454e-01,\n",
      "         1.1262e-01, -4.7654e-01, -4.1132e-01, -2.2667e-01, -6.8753e-01,\n",
      "        -4.0940e-01, -2.6463e-01, -1.6872e-01, -3.3353e-01, -1.0537e-01,\n",
      "        -1.5291e-01, -1.9922e-01, -4.9909e-01, -6.3670e-01, -4.2671e-01,\n",
      "        -2.5785e-01, -2.3476e-01, -3.5805e-01, -2.1716e-01, -1.6483e-01,\n",
      "        -1.2136e-01, -7.3447e-01,  6.9531e-02, -7.3833e-01, -5.3146e-01,\n",
      "        -2.9547e-01, -2.3935e-01, -1.1098e+00, -6.6346e-01, -4.5489e-01,\n",
      "        -6.1067e-01, -4.2938e-01, -4.2522e-01, -3.9158e-01, -4.3628e-01,\n",
      "        -1.4175e-01, -6.7278e-01, -6.1650e-01, -3.3496e-01, -3.4610e-01,\n",
      "        -3.5793e-01, -6.0118e-01, -5.4646e-01, -3.4308e-01, -2.5622e-01,\n",
      "        -3.7956e-01, -2.2628e-01, -8.2648e-01,  1.4650e-02, -5.3926e-01,\n",
      "        -5.8411e-01, -4.1610e-01, -4.7468e-01, -3.2089e-01, -4.7680e-01,\n",
      "        -1.9381e-01, -4.5869e-01, -1.8284e-01, -4.8120e-01, -2.6030e-01,\n",
      "        -4.1996e-01, -2.9099e-01, -2.4501e-01, -1.1521e-01, -3.8876e-01,\n",
      "        -6.7073e-01, -2.6060e-01, -3.1124e-01, -4.8611e-01, -2.2364e-01,\n",
      "        -6.2944e-01, -2.3952e-01, -4.6483e-01, -5.5043e-01, -4.4714e-01,\n",
      "        -4.3714e-01, -4.5892e-01, -2.6370e-01, -3.1202e-01, -6.2937e-01,\n",
      "        -2.6673e-01, -1.4085e-01, -3.6818e-01, -2.5663e-01, -3.0573e-01,\n",
      "        -4.1798e-01, -1.1895e-01, -4.8087e-01, -3.8435e-01, -3.4743e-01,\n",
      "        -4.2075e-01, -8.7884e-01, -3.4277e-01,  1.7818e-01, -5.9937e-01,\n",
      "        -5.2748e-01, -4.5096e-01, -6.7230e-01, -3.9579e-01, -7.0562e-01,\n",
      "        -3.0879e-01, -7.2413e-02, -4.1246e-01, -4.3837e-01, -9.1857e-01,\n",
      "        -8.9224e-01, -8.6363e-01, -1.1391e+00, -2.5436e-01, -4.6521e-01,\n",
      "        -4.8936e-01, -6.4422e-01, -1.2768e+00, -2.8899e-01, -5.0160e-01,\n",
      "        -3.5809e-01, -3.0336e-01, -2.3515e-01, -1.3725e-01, -7.5674e-01,\n",
      "        -4.0306e-01, -6.6200e-01, -2.6380e-01, -1.7746e-01, -5.5275e-01,\n",
      "        -7.4601e-01, -4.4341e-01, -2.5738e-01, -3.5780e-01, -4.1272e-01,\n",
      "        -2.2464e-01, -1.6649e-01, -2.6113e-01, -4.3488e-01, -3.5676e-01,\n",
      "        -2.5601e-02, -2.7972e-01, -2.7931e-01, -4.8900e-01, -4.0714e-01,\n",
      "        -4.4893e-01, -2.8465e-01, -4.0615e-01, -5.8578e-01, -1.4813e-01,\n",
      "        -2.3743e-01, -4.3030e-01, -1.6055e-01, -2.5200e-01, -5.1946e-02,\n",
      "        -1.8946e-01, -5.9929e-01, -1.0837e-01, -5.5855e-01, -8.8452e-02,\n",
      "        -3.2613e-01,  1.7746e-01, -7.4220e-02, -2.8766e-01, -8.4513e-01,\n",
      "        -4.0479e-01, -4.8621e-01, -4.1223e-01, -1.5384e-01, -4.7901e-01,\n",
      "        -6.5176e-01, -4.7152e-01, -2.1000e-01, -7.4626e-01, -5.9215e-01,\n",
      "        -4.6874e-01, -3.8679e-01, -3.5116e-01, -7.5207e-01, -7.2872e-01],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2915,  0.0950,  0.2058,  ..., -0.4331, -0.2418,  0.0202],\n",
      "        [-0.4446,  0.6365,  0.1951,  ..., -0.1886,  0.3395, -0.1013],\n",
      "        [-0.1653, -0.1922, -0.1577,  ...,  0.8673, -0.3950,  0.3082],\n",
      "        ...,\n",
      "        [-0.1595,  0.0340,  0.0072,  ...,  0.0962,  0.0657, -0.0739],\n",
      "        [-0.1179,  0.0120, -0.0529,  ...,  0.0573,  0.0908, -0.0931],\n",
      "        [-0.0130,  0.0089,  0.0294,  ..., -0.0195,  0.0227,  0.1171]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 1.4378, -0.7440,  0.4300,  0.1268,  0.2307, -0.2291, -0.6436, -0.6100,\n",
      "         0.0000,  0.0000], requires_grad=True)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/syft-0.2.6-py3.7.egg/syft/execution/placeholder.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/syft-0.2.6-py3.7.egg/syft/generic/abstract/tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mchild_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchild_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'grad'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e56d9e1fcd88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_plan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_autograd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/syft-0.2.6-py3.7.egg/syft/execution/plan.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, trace_autograd, *args)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mframework_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_framework_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mframework_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;31m# Register inputs in role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-2c7f0ce4bb62>\u001b[0m in \u001b[0;36mtraining_plan\u001b[0;34m(batch_size, lr, model_params)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m##gradient updates for each data entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msentence_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;34mr\"\"\"Sets gradients of all model parameters to zero.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Uni/susml/PySyft/venv/lib/python3.7/site-packages/syft-0.2.6-py3.7.egg/syft/execution/placeholder.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "# Dummy input parameters to make the trace\n",
    "model_params = list(model.parameters())\n",
    "print(model_params)\n",
    "X = th.randn(3, 28 * 28)\n",
    "y = nn.functional.one_hot(th.tensor([1, 2, 3]), 10)\n",
    "lr = th.tensor([0.01])\n",
    "batch_size = th.tensor([3.0])\n",
    "\n",
    "_ = training_plan.build(batch_size, lr, model_params, trace_autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysyft",
   "language": "python",
   "name": "pysyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
